{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e31b029bf6c0418a86c29fc1a5b01d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e09cb204dabe4f2ebb7ce752cd69891a",
              "IPY_MODEL_6557617d009c442d81b5cd4ca0048a72",
              "IPY_MODEL_882c746b304d4c07949e527f33692068"
            ],
            "layout": "IPY_MODEL_8c0f320a4361408ba235fedc22b114ff"
          }
        },
        "e09cb204dabe4f2ebb7ce752cd69891a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "Min PMI",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_267efa01a740489c85d9ed69ec934ff3",
            "max": 11,
            "min": -2,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".2f",
            "step": 0.5,
            "style": "IPY_MODEL_63eaa57cf26d40d2bd65e8f6b442c413",
            "value": 5
          }
        },
        "6557617d009c442d81b5cd4ca0048a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "Min Bigram Freq",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ffb3dd881a474a3199775c1570d856f3",
            "max": 150,
            "min": 1,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".2f",
            "step": 1,
            "style": "IPY_MODEL_0d5b0f00d9274e3fab46d04462af5332",
            "value": 5
          }
        },
        "882c746b304d4c07949e527f33692068": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_7f4edfd38b504a5bb3196cff7b0a2ba9",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "                        bigram          word1          word2  bigramFreq  \\\n502               (say, hello)            say          hello           5   \n918              (happy, bday)          happy           bday           6   \n3020         (fashion, friday)        fashion         friday           5   \n3530                    (‚ù§, Ô∏è)              ‚ù§              Ô∏è          13   \n3715      (bajrangi, bhaijaan)       bajrangi       bhaijaan           7   \n4703   (choice, international)         choice  international           8   \n5005       (bhaijaan, highest)       bhaijaan        highest           6   \n5026             (bulbs, made)          bulbs           made           5   \n5084          (start, working)          start        working          18   \n6528                  (pic, ‚Äî)            pic              ‚Äî           5   \n7418              (ball, pool)           ball           pool           5   \n11190      (much, appreciated)           much    appreciated           6   \n11504            (kind, words)           kind          words           6   \n12069          (final, design)          final         design          10   \n12788           (side, please)           side         please           6   \n13404            (corn, waste)           corn          waste           5   \n13780           (concert, let)        concert            let          17   \n14366          (super, junior)          super         junior           8   \n14946               (plz, let)            plz            let           8   \n15281  (international, artist)  international         artist           8   \n15295       (looking, forward)        looking        forward          14   \n16220             (next, year)           next           year           6   \n17751         (anything, else)       anything           else           5   \n18383            (can't, wait)          can't           wait          23   \n19110             (make, sure)           make           sure           9   \n19112            (last, night)           last          night          16   \n19450                   (üòÇ, üòÇ)              üòÇ              üòÇ           5   \n19937                   (‚ô°, ‚ô•)              ‚ô°              ‚ô•           5   \n20244            (wsale, love)          wsale           love           8   \n21837             (next, week)           next           week           5   \n22015             (take, care)           take           care           5   \n22824               (bi0, thx)            bi0            thx           7   \n22832          (look, forward)           look        forward           7   \n23205          (keep, smiling)           keep        smiling           6   \n24468                 (ha, ha)             ha             ha           7   \n24523           (follback, ya)       follback             ya           8   \n25014        (happy, birthday)          happy       birthday          40   \n26102           (light, bulbs)          light          bulbs           5   \n28695        (wicked, weekend)         wicked        weekend           8   \n28938         (highest, week1)        highest          week1           6   \n29145           (teen, choice)           teen         choice           9   \n29238            (i'll, start)           i'll          start          17   \n\n       word1Freq  word2Freq  bigramProb  word1Prob  word2Prob       pmi  \n502           39         39    0.000107   0.000834   0.000834  5.035184  \n918          164         11    0.000128   0.003507   0.000235  5.046867  \n3020           6        105    0.000107   0.000128   0.002245  5.916587  \n3530          24         19    0.000278   0.000513   0.000406  7.195326  \n3715           7          7    0.000150   0.000150   0.000150  8.806959  \n4703          24         10    0.000171   0.000513   0.000214  7.351672  \n5005           7          7    0.000128   0.000150   0.000150  8.652808  \n5026           5         51    0.000107   0.000107   0.001091  6.821043  \n5084          42         28    0.000385   0.000898   0.000599  6.573367  \n6528          14         26    0.000107   0.000299   0.000556  6.465153  \n7418           6          5    0.000107   0.000128   0.000107  8.961109  \n11190         85          9    0.000128   0.001818   0.000192  5.904753  \n11504         24         11    0.000128   0.000513   0.000235  6.968679  \n12069         13         16    0.000214   0.000278   0.000342  7.717916  \n12788         14         83    0.000128   0.000299   0.001775  5.486730  \n13404          5          6    0.000107   0.000107   0.000128  8.961109  \n13780         25         66    0.000364   0.000535   0.001411  6.177552  \n14366         23         10    0.000171   0.000492   0.000214  7.394231  \n14946         12         66    0.000171   0.000257   0.001411  6.157749  \n15281         10         10    0.000171   0.000214   0.000214  8.227140  \n15295         46         24    0.000299   0.000984   0.000513  6.385231  \n16220         46         30    0.000128   0.000984   0.000642  5.314790  \n17751         20          9    0.000107   0.000428   0.000192  7.169350  \n18383         42         42    0.000492   0.000898   0.000898  6.413024  \n19110         57         49    0.000192   0.001219   0.001048  5.015222  \n19112         40         58    0.000342   0.000855   0.001240  5.776135  \n19450         17         17    0.000107   0.000364   0.000364  6.695880  \n19937         14         19    0.000107   0.000299   0.000406  6.778811  \n20244          8        205    0.000171   0.000171   0.004384  5.429859  \n21837         46         30    0.000107   0.000984   0.000642  5.132468  \n22015         28         12    0.000107   0.000599   0.000257  6.545196  \n22824          8         14    0.000150   0.000171   0.000299  7.980280  \n22832         46         24    0.000150   0.000984   0.000513  5.692084  \n23205         55         13    0.000128   0.001176   0.000278  5.972346  \n24468         24         24    0.000150   0.000513   0.000513  6.342671  \n24523         19         17    0.000171   0.000406   0.000364  7.054658  \n25014        164         60    0.000855   0.003507   0.001283  5.247537  \n26102         13          5    0.000107   0.000278   0.000107  8.187920  \n28695          9         64    0.000171   0.000192   0.001369  6.476203  \n28938          7          6    0.000128   0.000150   0.000128  8.806959  \n29145         10         24    0.000192   0.000214   0.000513  7.469455  \n29238         86         42    0.000364   0.001839   0.000898  5.394065  ",
                  "text/html": "\n  <div id=\"df-e2055f5a-31db-4cbc-b0d6-81cb8c4e7b57\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bigram</th>\n      <th>word1</th>\n      <th>word2</th>\n      <th>bigramFreq</th>\n      <th>word1Freq</th>\n      <th>word2Freq</th>\n      <th>bigramProb</th>\n      <th>word1Prob</th>\n      <th>word2Prob</th>\n      <th>pmi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>502</th>\n      <td>(say, hello)</td>\n      <td>say</td>\n      <td>hello</td>\n      <td>5</td>\n      <td>39</td>\n      <td>39</td>\n      <td>0.000107</td>\n      <td>0.000834</td>\n      <td>0.000834</td>\n      <td>5.035184</td>\n    </tr>\n    <tr>\n      <th>918</th>\n      <td>(happy, bday)</td>\n      <td>happy</td>\n      <td>bday</td>\n      <td>6</td>\n      <td>164</td>\n      <td>11</td>\n      <td>0.000128</td>\n      <td>0.003507</td>\n      <td>0.000235</td>\n      <td>5.046867</td>\n    </tr>\n    <tr>\n      <th>3020</th>\n      <td>(fashion, friday)</td>\n      <td>fashion</td>\n      <td>friday</td>\n      <td>5</td>\n      <td>6</td>\n      <td>105</td>\n      <td>0.000107</td>\n      <td>0.000128</td>\n      <td>0.002245</td>\n      <td>5.916587</td>\n    </tr>\n    <tr>\n      <th>3530</th>\n      <td>(‚ù§, Ô∏è)</td>\n      <td>‚ù§</td>\n      <td>Ô∏è</td>\n      <td>13</td>\n      <td>24</td>\n      <td>19</td>\n      <td>0.000278</td>\n      <td>0.000513</td>\n      <td>0.000406</td>\n      <td>7.195326</td>\n    </tr>\n    <tr>\n      <th>3715</th>\n      <td>(bajrangi, bhaijaan)</td>\n      <td>bajrangi</td>\n      <td>bhaijaan</td>\n      <td>7</td>\n      <td>7</td>\n      <td>7</td>\n      <td>0.000150</td>\n      <td>0.000150</td>\n      <td>0.000150</td>\n      <td>8.806959</td>\n    </tr>\n    <tr>\n      <th>4703</th>\n      <td>(choice, international)</td>\n      <td>choice</td>\n      <td>international</td>\n      <td>8</td>\n      <td>24</td>\n      <td>10</td>\n      <td>0.000171</td>\n      <td>0.000513</td>\n      <td>0.000214</td>\n      <td>7.351672</td>\n    </tr>\n    <tr>\n      <th>5005</th>\n      <td>(bhaijaan, highest)</td>\n      <td>bhaijaan</td>\n      <td>highest</td>\n      <td>6</td>\n      <td>7</td>\n      <td>7</td>\n      <td>0.000128</td>\n      <td>0.000150</td>\n      <td>0.000150</td>\n      <td>8.652808</td>\n    </tr>\n    <tr>\n      <th>5026</th>\n      <td>(bulbs, made)</td>\n      <td>bulbs</td>\n      <td>made</td>\n      <td>5</td>\n      <td>5</td>\n      <td>51</td>\n      <td>0.000107</td>\n      <td>0.000107</td>\n      <td>0.001091</td>\n      <td>6.821043</td>\n    </tr>\n    <tr>\n      <th>5084</th>\n      <td>(start, working)</td>\n      <td>start</td>\n      <td>working</td>\n      <td>18</td>\n      <td>42</td>\n      <td>28</td>\n      <td>0.000385</td>\n      <td>0.000898</td>\n      <td>0.000599</td>\n      <td>6.573367</td>\n    </tr>\n    <tr>\n      <th>6528</th>\n      <td>(pic, ‚Äî)</td>\n      <td>pic</td>\n      <td>‚Äî</td>\n      <td>5</td>\n      <td>14</td>\n      <td>26</td>\n      <td>0.000107</td>\n      <td>0.000299</td>\n      <td>0.000556</td>\n      <td>6.465153</td>\n    </tr>\n    <tr>\n      <th>7418</th>\n      <td>(ball, pool)</td>\n      <td>ball</td>\n      <td>pool</td>\n      <td>5</td>\n      <td>6</td>\n      <td>5</td>\n      <td>0.000107</td>\n      <td>0.000128</td>\n      <td>0.000107</td>\n      <td>8.961109</td>\n    </tr>\n    <tr>\n      <th>11190</th>\n      <td>(much, appreciated)</td>\n      <td>much</td>\n      <td>appreciated</td>\n      <td>6</td>\n      <td>85</td>\n      <td>9</td>\n      <td>0.000128</td>\n      <td>0.001818</td>\n      <td>0.000192</td>\n      <td>5.904753</td>\n    </tr>\n    <tr>\n      <th>11504</th>\n      <td>(kind, words)</td>\n      <td>kind</td>\n      <td>words</td>\n      <td>6</td>\n      <td>24</td>\n      <td>11</td>\n      <td>0.000128</td>\n      <td>0.000513</td>\n      <td>0.000235</td>\n      <td>6.968679</td>\n    </tr>\n    <tr>\n      <th>12069</th>\n      <td>(final, design)</td>\n      <td>final</td>\n      <td>design</td>\n      <td>10</td>\n      <td>13</td>\n      <td>16</td>\n      <td>0.000214</td>\n      <td>0.000278</td>\n      <td>0.000342</td>\n      <td>7.717916</td>\n    </tr>\n    <tr>\n      <th>12788</th>\n      <td>(side, please)</td>\n      <td>side</td>\n      <td>please</td>\n      <td>6</td>\n      <td>14</td>\n      <td>83</td>\n      <td>0.000128</td>\n      <td>0.000299</td>\n      <td>0.001775</td>\n      <td>5.486730</td>\n    </tr>\n    <tr>\n      <th>13404</th>\n      <td>(corn, waste)</td>\n      <td>corn</td>\n      <td>waste</td>\n      <td>5</td>\n      <td>5</td>\n      <td>6</td>\n      <td>0.000107</td>\n      <td>0.000107</td>\n      <td>0.000128</td>\n      <td>8.961109</td>\n    </tr>\n    <tr>\n      <th>13780</th>\n      <td>(concert, let)</td>\n      <td>concert</td>\n      <td>let</td>\n      <td>17</td>\n      <td>25</td>\n      <td>66</td>\n      <td>0.000364</td>\n      <td>0.000535</td>\n      <td>0.001411</td>\n      <td>6.177552</td>\n    </tr>\n    <tr>\n      <th>14366</th>\n      <td>(super, junior)</td>\n      <td>super</td>\n      <td>junior</td>\n      <td>8</td>\n      <td>23</td>\n      <td>10</td>\n      <td>0.000171</td>\n      <td>0.000492</td>\n      <td>0.000214</td>\n      <td>7.394231</td>\n    </tr>\n    <tr>\n      <th>14946</th>\n      <td>(plz, let)</td>\n      <td>plz</td>\n      <td>let</td>\n      <td>8</td>\n      <td>12</td>\n      <td>66</td>\n      <td>0.000171</td>\n      <td>0.000257</td>\n      <td>0.001411</td>\n      <td>6.157749</td>\n    </tr>\n    <tr>\n      <th>15281</th>\n      <td>(international, artist)</td>\n      <td>international</td>\n      <td>artist</td>\n      <td>8</td>\n      <td>10</td>\n      <td>10</td>\n      <td>0.000171</td>\n      <td>0.000214</td>\n      <td>0.000214</td>\n      <td>8.227140</td>\n    </tr>\n    <tr>\n      <th>15295</th>\n      <td>(looking, forward)</td>\n      <td>looking</td>\n      <td>forward</td>\n      <td>14</td>\n      <td>46</td>\n      <td>24</td>\n      <td>0.000299</td>\n      <td>0.000984</td>\n      <td>0.000513</td>\n      <td>6.385231</td>\n    </tr>\n    <tr>\n      <th>16220</th>\n      <td>(next, year)</td>\n      <td>next</td>\n      <td>year</td>\n      <td>6</td>\n      <td>46</td>\n      <td>30</td>\n      <td>0.000128</td>\n      <td>0.000984</td>\n      <td>0.000642</td>\n      <td>5.314790</td>\n    </tr>\n    <tr>\n      <th>17751</th>\n      <td>(anything, else)</td>\n      <td>anything</td>\n      <td>else</td>\n      <td>5</td>\n      <td>20</td>\n      <td>9</td>\n      <td>0.000107</td>\n      <td>0.000428</td>\n      <td>0.000192</td>\n      <td>7.169350</td>\n    </tr>\n    <tr>\n      <th>18383</th>\n      <td>(can't, wait)</td>\n      <td>can't</td>\n      <td>wait</td>\n      <td>23</td>\n      <td>42</td>\n      <td>42</td>\n      <td>0.000492</td>\n      <td>0.000898</td>\n      <td>0.000898</td>\n      <td>6.413024</td>\n    </tr>\n    <tr>\n      <th>19110</th>\n      <td>(make, sure)</td>\n      <td>make</td>\n      <td>sure</td>\n      <td>9</td>\n      <td>57</td>\n      <td>49</td>\n      <td>0.000192</td>\n      <td>0.001219</td>\n      <td>0.001048</td>\n      <td>5.015222</td>\n    </tr>\n    <tr>\n      <th>19112</th>\n      <td>(last, night)</td>\n      <td>last</td>\n      <td>night</td>\n      <td>16</td>\n      <td>40</td>\n      <td>58</td>\n      <td>0.000342</td>\n      <td>0.000855</td>\n      <td>0.001240</td>\n      <td>5.776135</td>\n    </tr>\n    <tr>\n      <th>19450</th>\n      <td>(üòÇ, üòÇ)</td>\n      <td>üòÇ</td>\n      <td>üòÇ</td>\n      <td>5</td>\n      <td>17</td>\n      <td>17</td>\n      <td>0.000107</td>\n      <td>0.000364</td>\n      <td>0.000364</td>\n      <td>6.695880</td>\n    </tr>\n    <tr>\n      <th>19937</th>\n      <td>(‚ô°, ‚ô•)</td>\n      <td>‚ô°</td>\n      <td>‚ô•</td>\n      <td>5</td>\n      <td>14</td>\n      <td>19</td>\n      <td>0.000107</td>\n      <td>0.000299</td>\n      <td>0.000406</td>\n      <td>6.778811</td>\n    </tr>\n    <tr>\n      <th>20244</th>\n      <td>(wsale, love)</td>\n      <td>wsale</td>\n      <td>love</td>\n      <td>8</td>\n      <td>8</td>\n      <td>205</td>\n      <td>0.000171</td>\n      <td>0.000171</td>\n      <td>0.004384</td>\n      <td>5.429859</td>\n    </tr>\n    <tr>\n      <th>21837</th>\n      <td>(next, week)</td>\n      <td>next</td>\n      <td>week</td>\n      <td>5</td>\n      <td>46</td>\n      <td>30</td>\n      <td>0.000107</td>\n      <td>0.000984</td>\n      <td>0.000642</td>\n      <td>5.132468</td>\n    </tr>\n    <tr>\n      <th>22015</th>\n      <td>(take, care)</td>\n      <td>take</td>\n      <td>care</td>\n      <td>5</td>\n      <td>28</td>\n      <td>12</td>\n      <td>0.000107</td>\n      <td>0.000599</td>\n      <td>0.000257</td>\n      <td>6.545196</td>\n    </tr>\n    <tr>\n      <th>22824</th>\n      <td>(bi0, thx)</td>\n      <td>bi0</td>\n      <td>thx</td>\n      <td>7</td>\n      <td>8</td>\n      <td>14</td>\n      <td>0.000150</td>\n      <td>0.000171</td>\n      <td>0.000299</td>\n      <td>7.980280</td>\n    </tr>\n    <tr>\n      <th>22832</th>\n      <td>(look, forward)</td>\n      <td>look</td>\n      <td>forward</td>\n      <td>7</td>\n      <td>46</td>\n      <td>24</td>\n      <td>0.000150</td>\n      <td>0.000984</td>\n      <td>0.000513</td>\n      <td>5.692084</td>\n    </tr>\n    <tr>\n      <th>23205</th>\n      <td>(keep, smiling)</td>\n      <td>keep</td>\n      <td>smiling</td>\n      <td>6</td>\n      <td>55</td>\n      <td>13</td>\n      <td>0.000128</td>\n      <td>0.001176</td>\n      <td>0.000278</td>\n      <td>5.972346</td>\n    </tr>\n    <tr>\n      <th>24468</th>\n      <td>(ha, ha)</td>\n      <td>ha</td>\n      <td>ha</td>\n      <td>7</td>\n      <td>24</td>\n      <td>24</td>\n      <td>0.000150</td>\n      <td>0.000513</td>\n      <td>0.000513</td>\n      <td>6.342671</td>\n    </tr>\n    <tr>\n      <th>24523</th>\n      <td>(follback, ya)</td>\n      <td>follback</td>\n      <td>ya</td>\n      <td>8</td>\n      <td>19</td>\n      <td>17</td>\n      <td>0.000171</td>\n      <td>0.000406</td>\n      <td>0.000364</td>\n      <td>7.054658</td>\n    </tr>\n    <tr>\n      <th>25014</th>\n      <td>(happy, birthday)</td>\n      <td>happy</td>\n      <td>birthday</td>\n      <td>40</td>\n      <td>164</td>\n      <td>60</td>\n      <td>0.000855</td>\n      <td>0.003507</td>\n      <td>0.001283</td>\n      <td>5.247537</td>\n    </tr>\n    <tr>\n      <th>26102</th>\n      <td>(light, bulbs)</td>\n      <td>light</td>\n      <td>bulbs</td>\n      <td>5</td>\n      <td>13</td>\n      <td>5</td>\n      <td>0.000107</td>\n      <td>0.000278</td>\n      <td>0.000107</td>\n      <td>8.187920</td>\n    </tr>\n    <tr>\n      <th>28695</th>\n      <td>(wicked, weekend)</td>\n      <td>wicked</td>\n      <td>weekend</td>\n      <td>8</td>\n      <td>9</td>\n      <td>64</td>\n      <td>0.000171</td>\n      <td>0.000192</td>\n      <td>0.001369</td>\n      <td>6.476203</td>\n    </tr>\n    <tr>\n      <th>28938</th>\n      <td>(highest, week1)</td>\n      <td>highest</td>\n      <td>week1</td>\n      <td>6</td>\n      <td>7</td>\n      <td>6</td>\n      <td>0.000128</td>\n      <td>0.000150</td>\n      <td>0.000128</td>\n      <td>8.806959</td>\n    </tr>\n    <tr>\n      <th>29145</th>\n      <td>(teen, choice)</td>\n      <td>teen</td>\n      <td>choice</td>\n      <td>9</td>\n      <td>10</td>\n      <td>24</td>\n      <td>0.000192</td>\n      <td>0.000214</td>\n      <td>0.000513</td>\n      <td>7.469455</td>\n    </tr>\n    <tr>\n      <th>29238</th>\n      <td>(i'll, start)</td>\n      <td>i'll</td>\n      <td>start</td>\n      <td>17</td>\n      <td>86</td>\n      <td>42</td>\n      <td>0.000364</td>\n      <td>0.001839</td>\n      <td>0.000898</td>\n      <td>5.394065</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2055f5a-31db-4cbc-b0d6-81cb8c4e7b57')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n        \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n      \n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-e2055f5a-31db-4cbc-b0d6-81cb8c4e7b57 button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-e2055f5a-31db-4cbc-b0d6-81cb8c4e7b57');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n  "
                },
                "metadata": {}
              }
            ]
          }
        },
        "8c0f320a4361408ba235fedc22b114ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "267efa01a740489c85d9ed69ec934ff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "500px"
          }
        },
        "63eaa57cf26d40d2bd65e8f6b442c413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "ffb3dd881a474a3199775c1570d856f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "500px"
          }
        },
        "0d5b0f00d9274e3fab46d04462af5332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "7f4edfd38b504a5bb3196cff7b0a2ba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1645ad0128094958b991cd10a7c6ee21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_432dea32fa944d80af4a5dc2fc3d4efc",
              "IPY_MODEL_fe51921624744c7594a296a2bf1b1209",
              "IPY_MODEL_1caddc2b06e34ee292c5516192ee8275"
            ],
            "layout": "IPY_MODEL_f6b7cb8fd35f4c10a8a1c83b3451f8e2"
          }
        },
        "432dea32fa944d80af4a5dc2fc3d4efc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "Min PMI",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_a3ac1b819ba84d748d2c6fd6262993f0",
            "max": 11,
            "min": -2,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".2f",
            "step": 0.5,
            "style": "IPY_MODEL_31057007f8b74accbba94eba74f5e9cc",
            "value": 3
          }
        },
        "fe51921624744c7594a296a2bf1b1209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "Min Bigram Freq",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_83388a990b6d45ec8732d249f388a621",
            "max": 150,
            "min": 1,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".2f",
            "step": 1,
            "style": "IPY_MODEL_ce4cbd7e4b1e4892abe29e100a45e372",
            "value": 16
          }
        },
        "1caddc2b06e34ee292c5516192ee8275": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_75aa5cf6774643a886bb9b620276efad",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "               bigram   word1  word2  bigramFreq  word1Freq  word2Freq  \\\n2427     (i'm, still)     i'm  still          16        343        124   \n4616      (wanna, go)   wanna     go          24         94        139   \n5767   (coming, back)  coming   back          26         47        146   \n5874         („Äã, ÔΩìÔΩÖÔΩÖ)       „Äã    ÔΩìÔΩÖÔΩÖ          35        210         35   \n9444         (x15, „Äã)     x15      „Äã          35         35        210   \n11960    (ice, cream)     ice  cream          47         50         48   \n13087    (i'm, sorry)     i'm  sorry          26        343        150   \n14322          (‚ôõ, ‚ôõ)       ‚ôõ      ‚ôõ         141        210        210   \n18434  (can't, sleep)   can't  sleep          18        180         83   \n18622       (ÔΩìÔΩÖÔΩÖ, ÔΩçÔΩÖ)     ÔΩìÔΩÖÔΩÖ     ÔΩçÔΩÖ          35         35         35   \n20395   (last, night)    last  night          17         70         44   \n21886          („Äã, „Äã)       „Äã      „Äã         140        210        210   \n22759    (feel, like)    feel   like          29         99        206   \n24363          (‚ôõ, „Äã)       ‚ôõ      „Äã          35        210        210   \n24609    (come, back)    come   back          18         63        146   \n28640         (ÔΩçÔΩÖ, ‚ôõ)      ÔΩçÔΩÖ      ‚ôõ          35         35        210   \n\n       bigramProb  word1Prob  word2Prob       pmi  \n2427     0.000297   0.006357   0.002298  3.010464  \n4616     0.000445   0.001742   0.002576  4.596172  \n5767     0.000482   0.000871   0.002706  5.320229  \n5874     0.000649   0.003892   0.000649  5.548780  \n9444     0.000649   0.000649   0.003892  5.548780  \n11960    0.000871   0.000927   0.000890  6.962811  \n13087    0.000482   0.006357   0.002780  3.305618  \n14322    0.002613   0.003892   0.003892  5.150432  \n18434    0.000334   0.003336   0.001538  4.174461  \n18622    0.000649   0.000649   0.000649  7.340539  \n20395    0.000315   0.001297   0.000816  5.696416  \n21886    0.002595   0.003892   0.003892  5.143314  \n22759    0.000538   0.001835   0.003818  4.340187  \n24363    0.000649   0.003892   0.003892  3.757020  \n24609    0.000334   0.001168   0.002706  4.659518  \n28640    0.000649   0.000649   0.003892  5.548780  ",
                  "text/html": "\n  <div id=\"df-4732274c-d7f5-48cc-9366-8b1237b4a034\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bigram</th>\n      <th>word1</th>\n      <th>word2</th>\n      <th>bigramFreq</th>\n      <th>word1Freq</th>\n      <th>word2Freq</th>\n      <th>bigramProb</th>\n      <th>word1Prob</th>\n      <th>word2Prob</th>\n      <th>pmi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2427</th>\n      <td>(i'm, still)</td>\n      <td>i'm</td>\n      <td>still</td>\n      <td>16</td>\n      <td>343</td>\n      <td>124</td>\n      <td>0.000297</td>\n      <td>0.006357</td>\n      <td>0.002298</td>\n      <td>3.010464</td>\n    </tr>\n    <tr>\n      <th>4616</th>\n      <td>(wanna, go)</td>\n      <td>wanna</td>\n      <td>go</td>\n      <td>24</td>\n      <td>94</td>\n      <td>139</td>\n      <td>0.000445</td>\n      <td>0.001742</td>\n      <td>0.002576</td>\n      <td>4.596172</td>\n    </tr>\n    <tr>\n      <th>5767</th>\n      <td>(coming, back)</td>\n      <td>coming</td>\n      <td>back</td>\n      <td>26</td>\n      <td>47</td>\n      <td>146</td>\n      <td>0.000482</td>\n      <td>0.000871</td>\n      <td>0.002706</td>\n      <td>5.320229</td>\n    </tr>\n    <tr>\n      <th>5874</th>\n      <td>(„Äã, ÔΩìÔΩÖÔΩÖ)</td>\n      <td>„Äã</td>\n      <td>ÔΩìÔΩÖÔΩÖ</td>\n      <td>35</td>\n      <td>210</td>\n      <td>35</td>\n      <td>0.000649</td>\n      <td>0.003892</td>\n      <td>0.000649</td>\n      <td>5.548780</td>\n    </tr>\n    <tr>\n      <th>9444</th>\n      <td>(x15, „Äã)</td>\n      <td>x15</td>\n      <td>„Äã</td>\n      <td>35</td>\n      <td>35</td>\n      <td>210</td>\n      <td>0.000649</td>\n      <td>0.000649</td>\n      <td>0.003892</td>\n      <td>5.548780</td>\n    </tr>\n    <tr>\n      <th>11960</th>\n      <td>(ice, cream)</td>\n      <td>ice</td>\n      <td>cream</td>\n      <td>47</td>\n      <td>50</td>\n      <td>48</td>\n      <td>0.000871</td>\n      <td>0.000927</td>\n      <td>0.000890</td>\n      <td>6.962811</td>\n    </tr>\n    <tr>\n      <th>13087</th>\n      <td>(i'm, sorry)</td>\n      <td>i'm</td>\n      <td>sorry</td>\n      <td>26</td>\n      <td>343</td>\n      <td>150</td>\n      <td>0.000482</td>\n      <td>0.006357</td>\n      <td>0.002780</td>\n      <td>3.305618</td>\n    </tr>\n    <tr>\n      <th>14322</th>\n      <td>(‚ôõ, ‚ôõ)</td>\n      <td>‚ôõ</td>\n      <td>‚ôõ</td>\n      <td>141</td>\n      <td>210</td>\n      <td>210</td>\n      <td>0.002613</td>\n      <td>0.003892</td>\n      <td>0.003892</td>\n      <td>5.150432</td>\n    </tr>\n    <tr>\n      <th>18434</th>\n      <td>(can't, sleep)</td>\n      <td>can't</td>\n      <td>sleep</td>\n      <td>18</td>\n      <td>180</td>\n      <td>83</td>\n      <td>0.000334</td>\n      <td>0.003336</td>\n      <td>0.001538</td>\n      <td>4.174461</td>\n    </tr>\n    <tr>\n      <th>18622</th>\n      <td>(ÔΩìÔΩÖÔΩÖ, ÔΩçÔΩÖ)</td>\n      <td>ÔΩìÔΩÖÔΩÖ</td>\n      <td>ÔΩçÔΩÖ</td>\n      <td>35</td>\n      <td>35</td>\n      <td>35</td>\n      <td>0.000649</td>\n      <td>0.000649</td>\n      <td>0.000649</td>\n      <td>7.340539</td>\n    </tr>\n    <tr>\n      <th>20395</th>\n      <td>(last, night)</td>\n      <td>last</td>\n      <td>night</td>\n      <td>17</td>\n      <td>70</td>\n      <td>44</td>\n      <td>0.000315</td>\n      <td>0.001297</td>\n      <td>0.000816</td>\n      <td>5.696416</td>\n    </tr>\n    <tr>\n      <th>21886</th>\n      <td>(„Äã, „Äã)</td>\n      <td>„Äã</td>\n      <td>„Äã</td>\n      <td>140</td>\n      <td>210</td>\n      <td>210</td>\n      <td>0.002595</td>\n      <td>0.003892</td>\n      <td>0.003892</td>\n      <td>5.143314</td>\n    </tr>\n    <tr>\n      <th>22759</th>\n      <td>(feel, like)</td>\n      <td>feel</td>\n      <td>like</td>\n      <td>29</td>\n      <td>99</td>\n      <td>206</td>\n      <td>0.000538</td>\n      <td>0.001835</td>\n      <td>0.003818</td>\n      <td>4.340187</td>\n    </tr>\n    <tr>\n      <th>24363</th>\n      <td>(‚ôõ, „Äã)</td>\n      <td>‚ôõ</td>\n      <td>„Äã</td>\n      <td>35</td>\n      <td>210</td>\n      <td>210</td>\n      <td>0.000649</td>\n      <td>0.003892</td>\n      <td>0.003892</td>\n      <td>3.757020</td>\n    </tr>\n    <tr>\n      <th>24609</th>\n      <td>(come, back)</td>\n      <td>come</td>\n      <td>back</td>\n      <td>18</td>\n      <td>63</td>\n      <td>146</td>\n      <td>0.000334</td>\n      <td>0.001168</td>\n      <td>0.002706</td>\n      <td>4.659518</td>\n    </tr>\n    <tr>\n      <th>28640</th>\n      <td>(ÔΩçÔΩÖ, ‚ôõ)</td>\n      <td>ÔΩçÔΩÖ</td>\n      <td>‚ôõ</td>\n      <td>35</td>\n      <td>35</td>\n      <td>210</td>\n      <td>0.000649</td>\n      <td>0.000649</td>\n      <td>0.003892</td>\n      <td>5.548780</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4732274c-d7f5-48cc-9366-8b1237b4a034')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n        \n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n      \n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-4732274c-d7f5-48cc-9366-8b1237b4a034 button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-4732274c-d7f5-48cc-9366-8b1237b4a034');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n  "
                },
                "metadata": {}
              }
            ]
          }
        },
        "f6b7cb8fd35f4c10a8a1c83b3451f8e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ac1b819ba84d748d2c6fd6262993f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "500px"
          }
        },
        "31057007f8b74accbba94eba74f5e9cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "83388a990b6d45ec8732d249f388a621": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "500px"
          }
        },
        "ce4cbd7e4b1e4892abe29e100a45e372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "75aa5cf6774643a886bb9b620276efad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Partial Exam: Tweets Classification - Part III (25%)\n",
        "\n",
        "**Main Topics:** Tweet Classification and N-Grams analysis\n",
        "\n",
        "**Deadline:** April 15th 11:59 PM (Finnish Time)\n",
        "\n",
        "**Author:** Andr√©s Felipe Zapata Palacio  \n",
        "\n",
        "**Tasks:**  \n",
        "* #1 (40 Points) Model Evaluation\n",
        "* #2 (30 Points) Explore wrong Predictions\n",
        "* #3 (30 Points) Filter Bigrams\n",
        "\n",
        "üôã If you have any question related to this assignment, you can write it filling the following form: https://forms.gle/6aw7hVH7fKhgRqGLA"
      ],
      "metadata": {
        "id": "CjzyCsp9hw23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö†Ô∏è Important Information about the Submission Process ‚ö†Ô∏è\n",
        "\n",
        "You know the current restrictions that I have as Teacher at SAMK. Given that I don't have an institutional account, I cannot access in any way the Moodle platform. For this reason, I will use Dropbox to receive your exams. To do the following procedure you don't have to create a Dropbox account:\n",
        "\n",
        "1. Enter into the following Link: https://www.dropbox.com/request/qwN0AQmZnnFap2piUYwK\n",
        "\n",
        "2. Click on the button \"Add Files\", and then \"Files from your Computer\"\n",
        "\n",
        "3. Upload your .ipynb file\n",
        "\n",
        "4. If you are not logged in, the platform will ask you your name and your e-mail. Please, enter your Full Name and your institutional e-mail.\n",
        "\n",
        "\n",
        "If you have any trouble uploading the files, you MUST contact me. Don't wait until the deadline finishes."
      ],
      "metadata": {
        "id": "WEB3zpjw7Kk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Notes** ‚ö†Ô∏è\n",
        "\n",
        "* It's EXPLICITLY FORBIDDEN to use ChatGPT or any other Software to generate the answers or the analyses of the exam.\n",
        "\n",
        "* You are allowed to modify only the parts of the code that are delimited by the commentaries. These sections usually have a commentary that says: \"Write your code here\".\n",
        "\n",
        "* For the open questions, you have to explain your opinion or decision in detail. You must demonstrate that you dominate the topics of the course with your answer.\n",
        "\n",
        "* Upload the Notebook in Dropbox using the format ipynb, you must export the notebook without removing the outputs from the cells.\n",
        "\n",
        "* Verify that your notebook runs without errors before submitting it."
      ],
      "metadata": {
        "id": "tRWNOy-K75se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name of the Student (Penalization of 5 points)** ‚ö†Ô∏è\n",
        "\n",
        "One requirement for this assignment is to change the name of the file before uploading it to the system E.j (NLP_10_PartialExam_Andres_Zapata.ipynb). Additionally, you have to write your name in the space bellow:\n",
        "\n",
        "```\n",
        "Dawid Nalepa\n",
        "```\n",
        "\n",
        "If you don't do these two steps, your final score will be reduced in 5 points.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qk-pRKgj79LG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOfQN1YUAmKr"
      },
      "source": [
        "# Partial Exam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Auxiliar Functions and Dependencies ‚ö†Ô∏è\n",
        "#@markdown ‚ö° Run This cell to load the functions required for the exam, as well as all the dependencies and external libraries used in the process.\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Tweet Sample Dataset\n",
        "nltk.download('twitter_samples')\n",
        "\n",
        "# POS Tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Stop Words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Numpy\n",
        "import numpy as np\n",
        "\n",
        "# Regular Expressions\n",
        "import re\n",
        "\n",
        "# DataFrames\n",
        "import pandas as pd\n",
        "\n",
        "# Math\n",
        "import math\n",
        "\n",
        "# Interactive Widgets\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual, FloatSlider, Layout\n",
        "\n",
        "#Model Selection and Validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
        "\n",
        "def printTokensInVocabs(tokens):\n",
        "  counters = {'CountVectorizer': tfCounter,'TF Normalized':tfNormalizedCounter,'TfIdfVectorizer':tfIdfCounter}\n",
        "  for counterName in counters:\n",
        "    counter = counters[counterName]\n",
        "    newTokens = []\n",
        "    for token in tokenizedTweet:\n",
        "      if token in tfIdfCounter.vocabulary_:\n",
        "        newTokens.append(token)\n",
        "    print(f'Tokens in the Vocabulary of {counterName}: \\t{newTokens}')"
      ],
      "metadata": {
        "id": "J3jYaABABDMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a7cd59-3765-416a-a52d-584745f441c0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚òëÔ∏è Pre-Task 1.1: Load Dataset (0 Points)\n",
        "\n",
        "Load the Positive and Negative Tweets, and generate the labels for each sample."
      ],
      "metadata": {
        "id": "_qMyuf1xkFkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "positiveTweets = twitter_samples.strings('positive_tweets.json')\n",
        "negativeTweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "allTweets = []\n",
        "allTweets.extend(positiveTweets)\n",
        "allTweets.extend(negativeTweets)\n",
        "\n",
        "nPositive = len(positiveTweets)\n",
        "nNegative = len(negativeTweets)\n",
        "\n",
        "positiveLabels = np.ones(nPositive)\n",
        "negativeLabels = np.zeros(nNegative)\n",
        "\n",
        "allLabels = []\n",
        "allLabels.extend(positiveLabels)\n",
        "allLabels.extend(negativeLabels)\n",
        "allLabels = np.array(allLabels)"
      ],
      "metadata": {
        "id": "Y5jZBjGujBu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚òëÔ∏è Pre-Task 1.2: Clean and Process Data (0 Points)\n",
        "\n",
        "Define the different functions that will be used to clean and process the tweets, before transforming them into a numerical representation.\n",
        "\n",
        "* **preprocessTweet()** receives a tweet (string) and returns a cleaned string (Removes URLs, e-mails, mentions and repeated spaces)\n",
        "\n",
        "* **tokenizeTweet()** receives a tweet (string) and returns a list of tokens. This function splits the composed words. It uses the class TweetTokenizer, provided by NLTK library.\n",
        "\n",
        "* **cleanTokens()** receives a list of tokens and removes the tokens that are not needed (single punctuations, numbers, clean hashtag symbol, turns to lowercase)"
      ],
      "metadata": {
        "id": "IOj7rJXhkRii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessTweet(tweet):\n",
        "  tweet = re.sub('http[s]?://[\\S]+', ' ', tweet)              # Remove URLs\n",
        "  tweet = re.sub('[\\w]+([._-]\\w+)*@\\w+([.]\\w+)*', ' ', tweet) # Remove e-mails\n",
        "  tweet = re.sub('@\\S+','', tweet)                            # Remove mentions\n",
        "  tweet = re.sub('\\s+', ' ', tweet)                           # Replace repeated spaces to 1 single space\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "0gqh6Ni9kQo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanTokens(tokens):\n",
        "  newTokens = []\n",
        "  for token in tokens:\n",
        "    token = token.lower()\n",
        "    if re.match('^[_*#!$@<=^`>%&\\'\\\"/()\\[\\]\\-+,.:;?]$', token): # Remove tokens that are 1 single punctuation\n",
        "      continue\n",
        "    if re.match('\\d+', token): # Remove Numbers\n",
        "      continue\n",
        "    if re.match('#[\\w\\d]+', token): # Remove Hashtag\n",
        "      token = token[1:]\n",
        "    newTokens.append(token)\n",
        "  return newTokens"
      ],
      "metadata": {
        "id": "6PivHCXgkay5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitTokens(tokens):\n",
        "  splitPattern = r'(?<=[a-z])(?=[A-Z])'\n",
        "  newTokens = []\n",
        "  for token in tokens:\n",
        "    pieces = re.split(splitPattern, token)\n",
        "    newTokens.extend(pieces)\n",
        "  return newTokens"
      ],
      "metadata": {
        "id": "aOOrchL6kdLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "def tokenizeTweet(tweet):\n",
        "  tokens = TweetTokenizer().tokenize(tweet)\n",
        "  splittedTokens = splitTokens(tokens)\n",
        "  cleanedTokens = cleanTokens(splittedTokens)\n",
        "  return cleanedTokens"
      ],
      "metadata": {
        "id": "czJGvxEokfoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "englishStopWords = stopwords.words('english')"
      ],
      "metadata": {
        "id": "PgzCVd6YkmV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚òëÔ∏è Pre-Task 1.3: Generate Different Data Representation (0 Points)\n",
        "\n",
        "Create three different numerical representations for the Tweets dataset:\n",
        "\n",
        "1. Simple Term Frequency (Word Count) using CountVectorizer.\n",
        "2. Normalized Term Frequency using TfIdfVectorizer disabling IDF.\n",
        "3. TF-IDF Representation using TfIdfVectorizer."
      ],
      "metadata": {
        "id": "elSp-PTMkUpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "8JhiboDImWGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildVectorizers(max_features):\n",
        "  # Term Frequency\n",
        "  tfCounter = CountVectorizer(\n",
        "    preprocessor = preprocessTweet,\n",
        "    stop_words = englishStopWords,\n",
        "    tokenizer = tokenizeTweet,\n",
        "    max_features = max_features,\n",
        "  )\n",
        "  tfRepresentation = tfCounter.fit_transform(allTweets)\n",
        "\n",
        "  # TF Normalized\n",
        "  tfNormalizedCounter = TfidfVectorizer(\n",
        "    use_idf = False, norm = 'l2', # This removes the IDF part\n",
        "    preprocessor = preprocessTweet,\n",
        "    stop_words = englishStopWords,\n",
        "    tokenizer = tokenizeTweet,\n",
        "    max_features = max_features,\n",
        "  )\n",
        "  tfNormalizedRepresentation = tfNormalizedCounter.fit_transform(allTweets)\n",
        "\n",
        "  # TF-IDF Normalized\n",
        "  tfIdfCounter = TfidfVectorizer(\n",
        "    preprocessor = preprocessTweet,\n",
        "    stop_words = englishStopWords,\n",
        "    tokenizer = tokenizeTweet,\n",
        "    max_features = max_features,\n",
        "  )\n",
        "\n",
        "  tfIdfRepresentation = tfIdfCounter.fit_transform(allTweets)\n",
        "\n",
        "  return tfRepresentation, tfNormalizedRepresentation, tfIdfRepresentation, tfCounter, tfNormalizedCounter, tfIdfCounter"
      ],
      "metadata": {
        "id": "1s8TO4xHkq6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚òëÔ∏è Pre-Task 1.4: Train Function (0 Points)"
      ],
      "metadata": {
        "id": "ShEJkK-fdzJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainAndEvaluate(tweets, labels):\n",
        "  # Split Dataset in Train and Test\n",
        "  X_train, X_test, y_train, y_test = train_test_split(tweets, labels, shuffle=True, random_state=10)\n",
        "\n",
        "  # Build and Train the Model\n",
        "  model = LogisticRegressionCV(max_iter=2000)\n",
        "  model.fit(X_train,y_train)\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  trainAcc = model.score(X_train, y_train)\n",
        "  print(f'Train Accuracy: {trainAcc*100:.2f}%')\n",
        "  testAcc = model.score(X_test, y_test)\n",
        "  print(f'Test Accuracy: {testAcc*100:.2f}%\\n')\n",
        "\n",
        "  # Calculate other metrics\n",
        "  tn, fp, fn, tp = confusion_matrix(labels, model.predict(tweets)).ravel()\n",
        "  precision = tp / (tp + fp)\n",
        "  sensitivity = tp / (tp + fn)\n",
        "  specificity = tn / (tn + fp)\n",
        "  print(f'Precision: {precision*100:.2f}%')\n",
        "  print(f'Sensitivity: {sensitivity*100:.2f}%')\n",
        "  print(f'Specificity: {specificity*100:.2f}%')\n",
        "\n",
        "  # Return Variables\n",
        "  results = {\n",
        "      'model': model,\n",
        "      'testAcc':testAcc,\n",
        "      'trainAcc':trainAcc,\n",
        "      'precision':precision,\n",
        "      'sensitivity':sensitivity,\n",
        "      'specificity':specificity\n",
        "  }\n",
        "  return results"
      ],
      "metadata": {
        "id": "kqVY_bTDnY4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚òëÔ∏è Task 1: Model Evaluation (40 points)\n",
        "\n",
        "There are three different Logistic Regression models, each one is trained using a different numerical representation of the tweets.\n",
        "\n",
        "**üéØ Tasks:**\n",
        "\n",
        "* Explore different values for max_features. This variable can have values between 1 and 12.010 (Number of different tokens in the dataset after cleaning). This variable defines the size of the vocabulary that will be used during the training. Find a value that gives you the highest test accuracy and minimizes overfitting. What happens with the test and train accuracy when you have a very small value (5,20)? What happens with the test and train accuracy when you have a very big value (10.000, 12.000)?\n",
        "\n",
        "* Compare the performance of the three models and determine which of the three numerical representations is the best one for classifying Tweets in positive and negative using Logistic Regression.\n",
        "\n",
        "* Take into account all the different metrics given (accuracy, precision, sensitivity, specificity), and compare the performance over Training data and Test Data."
      ],
      "metadata": {
        "id": "C_5LNeiEkwoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "hBgqeSgJBAPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####   Write your code here   #####\n",
        "\n",
        "# Max Features must be between 1 and 12010\n",
        "max_features = 1000\n",
        "\n",
        "####################################\n",
        "\n",
        "tfRepresentation, tfNormalizedRepresentation, tfIdfRepresentation, tfCounter, tfNormalizedCounter, tfIdfCounter = buildVectorizers(max_features)\n",
        "print('\\n')\n",
        "print('Model (1) Absolute Term Frequency (Word Count)\\n')\n",
        "tfResults = trainAndEvaluate(tfRepresentation, allLabels)\n",
        "print('\\n')\n",
        "print('Model (2) Normalized Term Frequency (TF-IDF with IDF disabled)\\n')\n",
        "tfNormResults = trainAndEvaluate(tfNormalizedRepresentation, allLabels)\n",
        "print('\\n')\n",
        "print('Model (3) TF-IDF Representation\\n')\n",
        "tfIdfResults = trainAndEvaluate(tfIdfRepresentation, allLabels)\n",
        "\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCnbFCfRBrup",
        "outputId": "79916e30-5c30-44b4-d25c-3f3868b02df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Model (1) Absolute Term Frequency (Word Count)\n",
            "\n",
            "Train Accuracy: 99.97%\n",
            "Test Accuracy: 99.52%\n",
            "\n",
            "Precision: 99.74%\n",
            "Sensitivity: 99.98%\n",
            "Specificity: 99.74%\n",
            "\n",
            "\n",
            "Model (2) Normalized Term Frequency (TF-IDF with IDF disabled)\n",
            "\n",
            "Train Accuracy: 99.97%\n",
            "Test Accuracy: 99.64%\n",
            "\n",
            "Precision: 99.80%\n",
            "Sensitivity: 99.98%\n",
            "Specificity: 99.80%\n",
            "\n",
            "\n",
            "Model (3) TF-IDF Representation\n",
            "\n",
            "Train Accuracy: 99.91%\n",
            "Test Accuracy: 99.84%\n",
            "\n",
            "Precision: 99.84%\n",
            "Sensitivity: 99.94%\n",
            "Specificity: 99.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üß©Hints:**\n",
        "* Compare the test and the train accuracy to identify overfitting or underfitting.\n",
        "\n",
        "* Test accuracy reflects how well can a model generalize the learning from training set in data that is completly new.\n",
        "\n",
        "‚ÅâÔ∏è **Question (40 Points)** üßê\n",
        "\n",
        "What did you observe with the different values of max_features? Did you observe overfitting or underfitting with any group of values?\n",
        "\n",
        "```\n",
        "When I have initially set the values between (5,20), I have\n",
        "noticed that the model was underfitting the train and test accuracy.\n",
        "In cases of setting th max_features to anything above 10, the train accuracy\n",
        "would become higher than the test accuracy.\n",
        "\n",
        "When setting the value of max_features between (10000,12000), I arrived to\n",
        "a conclusion that the models are overfitting the train and test accuracy.\n",
        "The train values were slowly getting closer to 100% meanwhile the test accuracy\n",
        "was balancing itself within the 99.5% range, increasing by .1% on each model.\n",
        "```\n",
        "\n",
        "Which value for max_features gives you the best training results?\n",
        "\n",
        "```\n",
        "Setting max_features to 12,000 gave the best training results.\n",
        "When I have set the max_features to 10,000 it gave a 100% training result on only the first 2 models.\n",
        "Where as setting the max_features to 12,000 it gave 100% across all 3 models.\n",
        "```\n",
        "\n",
        "Which of the three models is the best one (TF, TF-normalized, TF-IDF)? You must give arguments based on the different metrics (precision, sensitivity, specificity) and you must compare also the Test and Train accuracy.\n",
        "\n",
        "```\n",
        "TF-IDF is the best out of the three models presented.\n",
        "\n",
        "It had an outstanding perforance for train and test splits\n",
        "by having the highest accuracy score when compared to the other two models.\n",
        "\n",
        "It also gives a high precision percentage which will prove effective when\n",
        "classifying positive and negative tweets.\n",
        "\n",
        "In terms of sensitivity, it is higher in comparisment to the other models\n",
        "giving it a winning edge at identifying positive and negative tweets.\n",
        "\n",
        "As for specificity, it has a lower chance of classyfing positive and negative\n",
        "tweets incorrectly.\n",
        "```"
      ],
      "metadata": {
        "id": "uIkV_Pvesv4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚òëÔ∏è Task 2: Explore Wrong Predictions (30 Points)\n",
        "\n",
        "**üéØTask:** The following code cells provide the list of tweets that were misclassified by each one of the three models. You must see them in detail and try to determine possible reasons for these misclassifications. You must pay attention to the preprocessing, the cleaning and the processing stages. It's possible that some tweets contain details that make them difficult to classify. You must also verify if the words are present in the vocabulary of each Vectorizer."
      ],
      "metadata": {
        "id": "ZOIjvyK2vDWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainAndSeeErrors(model, tweets, labels):\n",
        "  matrix = tweets.toarray()\n",
        "  for i in range(len(labels)):\n",
        "    pred = model.predict([matrix[i]])\n",
        "    real = labels[i]\n",
        "    if pred != real:\n",
        "      print('---------')\n",
        "      if real == 1: label = '(+)'\n",
        "      else: label = '(-)'\n",
        "      print(f'{label} -> {allTweets[i]}')"
      ],
      "metadata": {
        "id": "0U8gXt01vGj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainAndSeeErrors(tfResults['model'], tfRepresentation, allLabels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k3NLmqdvGVE",
        "outputId": "deb8150a-89fe-442f-ddcd-e16dab4a62a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------\n",
            "(+) -> @ellekagaoan @chinmarquez Catch up once in a while :( &gt;:D&lt; @aditriphosphate @ErinMonzon\n",
            "---------\n",
            "(-) -> @sainsburys guys a really unlucky one. The driver and I briefly checked eggs but my other half spotted this : ( http://t.co/WpCqJHhBVk\n",
            "---------\n",
            "(-) -> all time looww(:(\n",
            "---------\n",
            "(-) -> stu is mean, i just wanna sleep : (\n",
            "---------\n",
            "(-) -> @PSYCRM you still haven't ! ! : (\n",
            "---------\n",
            "(-) -> @c_tuilagi Anytime Lil Nigga!! (: (:\n",
            "---------\n",
            "(-) -> 20 losing streak... sad (:-(\n",
            "---------\n",
            "(-) -> i pOPPED CONFETTI THOUGH ! ! : ( https://t.co/Y79gPDxTIE\n",
            "---------\n",
            "(-) -> Zehr khany ka time is coming soon.....: (\n",
            "---------\n",
            "(-) -> Annnd, now not going to Winchester {:-(\n",
            "---------\n",
            "(-) -> pats jay : (\n",
            "---------\n",
            "(-) -> my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "---------\n",
            "(-) -> @CHEDA_KHAN Thats life. I get calls from people I havent seen in 20 years and its always favours : (\n",
            "---------\n",
            "(-) -> Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainAndSeeErrors(tfNormResults['model'], tfNormalizedRepresentation, allLabels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7OdPZ9avzjQ",
        "outputId": "03be32ab-40ff-47e9-8e5c-9513deb45a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------\n",
            "(+) -> @ellekagaoan @chinmarquez Catch up once in a while :( &gt;:D&lt; @aditriphosphate @ErinMonzon\n",
            "---------\n",
            "(-) -> all time looww(:(\n",
            "---------\n",
            "(-) -> stu is mean, i just wanna sleep : (\n",
            "---------\n",
            "(-) -> @c_tuilagi Anytime Lil Nigga!! (: (:\n",
            "---------\n",
            "(-) -> i pOPPED CONFETTI THOUGH ! ! : ( https://t.co/Y79gPDxTIE\n",
            "---------\n",
            "(-) -> Zehr khany ka time is coming soon.....: (\n",
            "---------\n",
            "(-) -> Annnd, now not going to Winchester {:-(\n",
            "---------\n",
            "(-) -> pats jay : (\n",
            "---------\n",
            "(-) -> my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "---------\n",
            "(-) -> @CHEDA_KHAN Thats life. I get calls from people I havent seen in 20 years and its always favours : (\n",
            "---------\n",
            "(-) -> Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainAndSeeErrors(tfIdfResults['model'], tfIdfRepresentation, allLabels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FciNHdG8vzUE",
        "outputId": "c0237555-127c-48e2-c7e7-9a2a6144c80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------\n",
            "(+) -> Remember that one time I didn't go to flume/kaytranada/alunageorge even though I had tickets? I still want to kms. : ) : )\n",
            "---------\n",
            "(+) -> FNAF 4 dropped...looks like no sleep 4 me : )))))\n",
            "---------\n",
            "(+) -> @ellekagaoan @chinmarquez Catch up once in a while :( &gt;:D&lt; @aditriphosphate @ErinMonzon\n",
            "---------\n",
            "(-) -> @Israelgirly They sure do, esp now when ppl are talking crap about Millie!! &gt;:( I'll go straight to that FB page:)\n",
            "---------\n",
            "(-) -> @wtfxmbs AMBS please it's harry's jeans :)):):):(\n",
            "---------\n",
            "(-) -> i pOPPED CONFETTI THOUGH ! ! : ( https://t.co/Y79gPDxTIE\n",
            "---------\n",
            "(-) -> @Mickb1980 @CalderClarion @ev2cycling Looks good pal. Glad I paid ¬£111 for my jersey and gilet! : (\n",
            "---------\n",
            "(-) -> Annnd, now not going to Winchester {:-(\n",
            "---------\n",
            "(-) -> pats jay : (\n",
            "---------\n",
            "(-) -> my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "---------\n",
            "(-) -> Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####################       WRITE YOUR CODE HERE       #####################\n",
        "myTestTweet = '''\n",
        "Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n",
        "'''\n",
        "############################################################################\n",
        "myTestTweet = myTestTweet.replace('\\n',' ')\n",
        "preprocessedTweet = preprocessTweet(myTestTweet)\n",
        "tokenizedTweet = tokenizeTweet(preprocessedTweet)\n",
        "\n",
        "print(f'Original Tweet:\\t\\t{myTestTweet}')\n",
        "print(f'Pre-processed Tweet:\\t{preprocessedTweet}')\n",
        "print(f'Tokenized Tweet:\\t{tokenizedTweet}')\n",
        "printTokensInVocabs(tokenizedTweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT7NHGb0wj9y",
        "outputId": "2de6950c-cf52-460c-bf55-8417abe5b379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tweet:\t\t Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring \n",
            "Pre-processed Tweet:\t Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) #Finance #ExpediaJobs #Job #Jobs #Hiring \n",
            "Tokenized Tweet:\t['sr', 'financial', 'analyst', 'expedia', 'inc', 'bellevue', 'wa', 'finance', 'expedia', 'jobs', 'job', 'jobs', 'hiring']\n",
            "Tokens in the Vocabulary of CountVectorizer: \t['job']\n",
            "Tokens in the Vocabulary of TF Normalized: \t['job']\n",
            "Tokens in the Vocabulary of TfIdfVectorizer: \t['job']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ÅâÔ∏è **Question (30 Points)** üßê\n",
        "\n",
        "What did you observe exploring the tweets that were misclassified? Is there a problem in the preprocessing? Is there a problem in the tokenization? Is it a problem of the model? Are there samples that have a wrong label?\n",
        "\n",
        "```\n",
        "When inspecting each individual misclassified tweet I came to a realizations\n",
        "that they shared similar characters, which was a sad emoji.\n",
        "I believe that they are misclassified due to that specific reason, as the models\n",
        "see the emoji and classify it as negative due to nature of the emoji.\n",
        "In some cases the order in which the happy emoji is written has been reversed.\n",
        "Example, original = \":)\", in some tweets = \"(:\".\n",
        "This could also possibly lead to misclassification.\n",
        "\n",
        "The pre-processing seems to be working perfectly, it removes mentions from tweets,\n",
        "as well as hyperlinks. In my opinion pre-processing could handle emojis significantly\n",
        "better as it gives it gives errors during tokenization.\n",
        "\n",
        "The tokenization seems to ignore sign emojis, this may be caused due to\n",
        "whitespace in between the two signs. While tokenizing the tweets, the cleaning\n",
        "process appears to be cleaning it incorrectly. For example, in one of the Tweets,\n",
        "the word \"pOPPED\" has been played into the tokenized as following [\"p\",\"opped\"].\n",
        "The words even though it should be have been kept together is being split due\n",
        "to the splitting function, in which words are split if they consist of lower and\n",
        "upper case.\n",
        "\n",
        "I do not believe it is an issue with the model.\n",
        "\n",
        "In my personal opinion I believe that there are samples which have been labeled\n",
        "incorrectly. In most cases it is positive tweets being classified as negative tweets.\n",
        "```\n",
        "\n",
        "What would you do to correct those misclassifications?\n",
        "\n",
        "```\n",
        "I believe that the correct course of action would be to clarify and debug the tokenization\n",
        "process in order to avoid issues such as incorrect splitting of words or ignoring emojis.\n",
        "```"
      ],
      "metadata": {
        "id": "csbZGG1DcjyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚òëÔ∏è Task 3: Filter Bigrams (30 points)\n",
        "\n",
        "**NOTE:** The answers for each task must be located in the text cell at the bottom.\n",
        "\n",
        "**üéØ Task 3.1:** Explore the bigrams extracted from the positive tweets, and define which are the 10 most relevant bigrams of positive tweets. You must take into account different factors, such as Bigram Frequency, Bigram PMI, How meaningful are them, How important are them to determine if a tweet is positive or not.\n",
        "\n",
        "**üéØ Task 3.2:** Explain the process that you performed to get your list of 10 most important positive bigrams. You have two options to get this list: the first option is using the interactive cell. You can filter the bigrams setting a minimum PMI, a minimum Bigram Frequency and specifying a list of words that you want to remove. The second option is exploring the entire dataframe and the full list of bigrams and filtering it manually using Python code, and defining your own criteria.\n",
        "\n",
        "**üéØ Task 3.3:** Explore the bigrams extracted from the negative tweets, and define which are the 10 most relevant bigrams of negative tweets. You must take into account different factors, such as Bigram Frequency, Bigram PMI, How meaningful are them, How important are them to determine if a tweet is negative or not.\n",
        "\n",
        "**üéØ Task 3.4:** Explain the process that you performed to get your list of 10 most important negative bigrams. You have two options to get this list: the first option is using the interactive cell. You can filter the bigrams setting a minimum PMI, a minimum Bigram Frequency and specifying a list of words that you want to remove. The second option is exploring the entire dataframe and the full list of bigrams and filtering it manually using Python code, and defining your own criteria."
      ],
      "metadata": {
        "id": "p4z-jXX2m2-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Auxiliar Functions ‚ö†Ô∏è\n",
        "#@markdown ‚ö° Run this cell to load the functions that will allow you to filter the bigrams\n",
        "\n",
        "def calculatePMI(data):\n",
        "  word1Prob = data['word1Prob']\n",
        "  word2Prob = data['word2Prob']\n",
        "  bigramProb = data['bigramProb']\n",
        "  pmi = math.log(bigramProb/(word1Prob*word2Prob))\n",
        "  return pmi\n",
        "\n",
        "def filterStopWords(data,stopWords):\n",
        "  word1 = data['word1']\n",
        "  word2 = data['word2']\n",
        "  if word1 in stopWords or word1 in englishStopWords:\n",
        "    return False\n",
        "  if word2 in stopWords or word2 in englishStopWords:\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "def getBigramIndicators(tweets):\n",
        "  tweets = ' '.join(tweets)\n",
        "  tweets = preprocessTweet(tweets)\n",
        "  words = tokenizeTweet(tweets)\n",
        "  bigrams = nltk.bigrams(words)\n",
        "  bigrams = list(bigrams)\n",
        "  bigramFrequencies = nltk.FreqDist(bigrams)\n",
        "  wordFrequencies = nltk.FreqDist(words)\n",
        "  df = pd.DataFrame()\n",
        "  df['bigram'] = list(set(bigrams))\n",
        "  df['word1'] = df['bigram'].apply(lambda bigram: bigram[0])\n",
        "  df['word2'] = df['bigram'].apply(lambda bigram: bigram[1])\n",
        "  df['bigramFreq'] = df['bigram'].apply(lambda bigram: bigramFrequencies[bigram])\n",
        "  df['word1Freq'] = df['word1'].apply(lambda word1: wordFrequencies[word1])\n",
        "  df['word2Freq'] = df['word2'].apply(lambda word2: wordFrequencies[word2])\n",
        "  df['bigramProb'] = df['bigramFreq'].apply(lambda freq: freq/len(bigrams))\n",
        "  df['word1Prob'] = df['word1Freq'].apply(lambda wordFreq: wordFreq/len(words))\n",
        "  df['word2Prob'] = df['word2Freq'].apply(lambda wordFreq: wordFreq/len(words))\n",
        "  df['pmi'] = df.apply(lambda data: calculatePMI(data), axis=1)\n",
        "  return df\n",
        "\n",
        "def removeRepeatedTweets(tweets):\n",
        "  beginnings = set()\n",
        "\n",
        "  uniqueTweets = []\n",
        "\n",
        "  for tweet in tweets:\n",
        "    beginning = tweet[:10]\n",
        "    if beginning not in beginnings:\n",
        "      uniqueTweets.append(tweet)\n",
        "      beginnings.add(beginning)\n",
        "\n",
        "  return uniqueTweets\n",
        "\n",
        "uniquePositive = removeRepeatedTweets(positiveTweets)\n",
        "uniqueNegative = removeRepeatedTweets(negativeTweets)"
      ],
      "metadata": {
        "id": "zPBbpqB7zPL_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filterBigrams(dfBigrams, minPMI, minBigramFreq):\n",
        "  df = dfBigrams\n",
        "  df = df[df['bigramFreq'] >= minBigramFreq]\n",
        "  df = df[df['pmi'] >= minPMI]\n",
        "  return df"
      ],
      "metadata": {
        "id": "rQY-IjZ4oVYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positiveBigrams = getBigramIndicators(positiveTweets)\n",
        "negativeBigrams = getBigramIndicators(negativeTweets)\n",
        "allBigrams = getBigramIndicators(allTweets)"
      ],
      "metadata": {
        "id": "no2HHnmKjzCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Analize Bigrams from positive Tweets (Option 1)\n",
        "#@markdown ‚ö° Run this cell to filter the different bigrams, specitying minimum values for PMI and Bigram Frequency.\n",
        "#@markdown You can write multiple words to be filtered in the text box separating them by a comma.\n",
        "\n",
        "# Interactive Controls\n",
        "minPMI = FloatSlider(min=-2, max=11, step=0.5, value=-2,description='Min PMI',layout=Layout(width='500px'))\n",
        "minBigramFreq = FloatSlider(min=1, max=150, step=1, value=1,description='Min Bigram Freq',layout=Layout(width='500px'))\n",
        "WordsToRemove = \":),:-),u,:d,:p,yet,the,and,an,a,good,follow\" #@param {type:\"string\"}\n",
        "\n",
        "positiveBigrams = getBigramIndicators(uniquePositive)\n",
        "stopWords = WordsToRemove.replace(' ','').split(',')\n",
        "\n",
        "@interact\n",
        "def filterPositive(minPMI=minPMI, minBigramFreq=minBigramFreq):\n",
        "  df = filterBigrams(positiveBigrams, minPMI, minBigramFreq)\n",
        "  return df[df.apply(lambda data: filterStopWords(data, stopWords), axis=1)]\n",
        "\n",
        "#filterPositive(minPMI=minPMI, minBigramFreq=minBigramFreq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e31b029bf6c0418a86c29fc1a5b01d27",
            "e09cb204dabe4f2ebb7ce752cd69891a",
            "6557617d009c442d81b5cd4ca0048a72",
            "882c746b304d4c07949e527f33692068",
            "8c0f320a4361408ba235fedc22b114ff",
            "267efa01a740489c85d9ed69ec934ff3",
            "63eaa57cf26d40d2bd65e8f6b442c413",
            "ffb3dd881a474a3199775c1570d856f3",
            "0d5b0f00d9274e3fab46d04462af5332",
            "7f4edfd38b504a5bb3196cff7b0a2ba9"
          ]
        },
        "id": "RDqpZDblptOO",
        "outputId": "2ba0279d-a9b8-4f5f-af3e-90353ebabe58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "interactive(children=(FloatSlider(value=-2.0, description='Min PMI', layout=Layout(width='500px'), max=11.0, m‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e31b029bf6c0418a86c29fc1a5b01d27"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to filter the bigrams by yourself and explore them in more detail, you can use the following cell of code to do your own filters and calculations. ‚ö†Ô∏è DON'T MODIFY positiveBigrams DIRECTLY, MODIFY df instead."
      ],
      "metadata": {
        "id": "WJ6HaEO6h1mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Write your code here #######\n",
        "\n",
        "# Use this cell to explore the positiveTweets to validate that the bigrams you chose are really relevant or not\n",
        "\n",
        "data = positiveTweets\n",
        "data[90:150]\n",
        "\n",
        "####################################"
      ],
      "metadata": {
        "id": "rQhn6qIBnCdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffbda128-c8fc-413e-d9b5-0d6c23887c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I added a video to a @YouTube playlist http://t.co/HVVPhSYakA im back on twitch and today it going to be league :) - 1 / 3',\n",
              " '#FollowFriday @AmericanOGrain @PecomeP @APaulicand for being top supports in my community this week :)',\n",
              " '@ZaynZaynmalik30  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)',\n",
              " \"Gym Monday can't wait :). Likes\",\n",
              " \"@HarNiLiZaLouis Hey, here's your invite to join Scope as an influencer :)  http://t.co/rZgZtQ2fJT\",\n",
              " 'Those friends know themselves :)',\n",
              " 'waiting for nudes :-)',\n",
              " '@JacobWhitesides go sleep u ! :)))))))))',\n",
              " 'Stats for the day have arrived. 1 new follower and NO unfollowers :) via http://t.co/RB8pMNgMEo.',\n",
              " 'My birthday is a week today! :D',\n",
              " \"@metalgear_jp @Kojima_Hideo I want you're T-shirts ! They are so cool ! :D\",\n",
              " '@AxeRade haw phela if am not looking like Mom obviously am looking like him :)',\n",
              " '@zaynmalik prince charming on stage :) x https://t.co/OnVFhzt5fZ',\n",
              " 'i have really good luck :)',\n",
              " 'Stats for the day have arrived. 1 new follower and NO unfollowers :) via http://t.co/XzsOGaC4zK.',\n",
              " '#FollowFriday @straz_das @DCarsonCPA @GH813600 for being top engaged members in my community this week :)',\n",
              " '@twentyonepilots @fujirock_jp tylers hipster glasses :D',\n",
              " '@MartyRafenstein \\n\\nHey Marty !! Glad to see on Twitter. :)',\n",
              " '@UKBusinessLunch Hi we will be joining you again today :)',\n",
              " '@LAfitnessUKhelp done thanks :-)',\n",
              " 'Its already afternoon, lets read Al Kahfi before the day finish :)',\n",
              " 'OhmyG! yaya dub @mainedcm im done doing to stalk your ig accnt.. GONDOOO MOO TOLOGOOO :) HAHA',\n",
              " \"@rozbabes Here's your invite to become a Scope influencer :) Details here: http://t.co/ipJ2yOiGet\",\n",
              " '#FollowFriday @NGourd @Locita @D_Robert_Kelly for being top influencers in my community this week :)',\n",
              " '@kevinthewhippet @Cassie_Spaniel @Bracken_Nelson @BellisimoBella1 @SpanielHarry thanks :) zzz xx',\n",
              " 'Physiotherapy Friday is my hashtag for today. Custom! :-)',\n",
              " '@Cjlopez21 you know I will üí™ Monica and I miss you to, yeah sounds good to me :)',\n",
              " '\"Good morning, beautiful :)\" That\\'s all it takes.',\n",
              " 'Hi BAM ! @BarsAndMelody \\nCan you follow my bestfriend @969Horan696 ? \\nShe loves you a lot :) \\nSee you in Warsaw &lt;3 \\nLove you &lt;3 x43',\n",
              " '@MandaScapinello oh yeah?? I am definitely going to try it tonight then! :) we took your advice too! We are in Treviso now!',\n",
              " '@morallosanthony  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)',\n",
              " \"@NIKKIERIOZZI Hi! Would you like an @imPastel concert? Let me know your city and country and I'll start working on it! Thanks :)\",\n",
              " 'Fine, \\n...Have a gorgeous Friday, friend!!..; xo..-:)) https://t.co/JmpkZP2DaI',\n",
              " 'Oven roasted garlic in olive oil, sun dried tomatoes, some dried basil, and century tuna :) https://t.co/EsCc9QhLob',\n",
              " '@HostMyOffice @NigelPWhittaker @lemezma @TWBC_Business @_TheBunkerJL Right back atchya!  Have a great day everyone :)',\n",
              " \"@TeamTall17 @FlashHayer  but doesn't even follow back :-)\",\n",
              " 'The future is almost here then :) https://t.co/rolE3ZCL97',\n",
              " '#FollowFriday @Michelploria @MyFrenchCity @jasoncreation for being top new followers in my community this week :)',\n",
              " 'No chance :)',\n",
              " '@ChiAB2486 @MrCliveC @PCDKirkwood @DC_ARVSgt @COPS_President @EmWilliamsCCCU @LauraRGallagher @Hall11Kate @LaMinx541 @JohnTarbet71 cheers :)',\n",
              " '@thatguycalledP go po for ice cream :))',\n",
              " '@19strawberry66 I agree 100% :)',\n",
              " '@SpazzyTsukihara hehehehe thats the point :p',\n",
              " '@KalinWhite next time just stay home :)',\n",
              " '@Gculloty87 thanks :)',\n",
              " '#FollowFriday @digitalplace2be @intlboost @_lafontpresse for being top supports in my community this week :)',\n",
              " '@stayfaboo it will be up soon, i promise :)',\n",
              " 'Web Whatsapp volta a funcionar com iPhone jailbroken. :-)',\n",
              " '@crustyolddeen (I plan on watching it later) at 34 mins Leia appears as a hologram from R2D2 w a message for Obi Wan as he sits w Luke :)',\n",
              " '@Mburu__ Inter 3 UCL, Arsenal... Small team, Right! :)',\n",
              " 'Just passing by :) üöÇ (@ Dewsbury Railway Station (DEW) - @nationalrailenq in Dewsbury, West Yorkshire) https://t.co/DvBssHbrfx',\n",
              " '@ClearlyArticle :) its 430 am smh',\n",
              " \"@uptommosass it's 9:25 am here :) I live in Scotland :) it's strange to imagine what?- Megan xx\",\n",
              " '@bookmyshow \\n\\n !\\n#MasaanToday\\n#MasaanToday\\n\\nA4.... Shweta Tripathi  \\n !\\n#MasaanToday\\n#MasaanToday\\n\\n-.:-) .-..,',\n",
              " '@WforWoman 5. Over 20 W kurtas! And my Mom has about half the number I have :D #WSaleLove',\n",
              " 'Ah! Hello back Larry! @TransworldBooks :) https://t.co/8XhjJb4jtH',\n",
              " 'Anyway my Friday is looking kinda gooood :)',\n",
              " '@jhun_hunyo  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)',\n",
              " \"Lol. Well. That's life :) thank God it enn you  https://t.co/GLoCEIjGQQ\",\n",
              " '@OJBJ @holmesjsamuel surely this could be your warmup this morning ??? Coming on 15th to Bath :) xx']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####### Write your code here #######\n",
        "\n",
        "# Use this cell to explore the DataFrame that contains the Bigrams present in the Positive Tweets\n",
        "\n",
        "df = positiveBigrams\n",
        "df[70:120]\n",
        "\n",
        "####################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JoiTtRU1qKOX",
        "outputId": "0883e41b-2170-4b76-f000-ecf305e5c45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      bigram     word1        word2  bigramFreq  word1Freq  \\\n",
              "70           (almost, views)    almost        views           1         10   \n",
              "71                  (me, ;))        me           ;)           1        324   \n",
              "72                (now, :-))       now          :-)           5        120   \n",
              "73                (or, blue)        or         blue           1         60   \n",
              "74           (for, tonights)       for     tonights           1        619   \n",
              "75             (good, start)      good        start           1        210   \n",
              "76               (of, maple)        of        maple           1        379   \n",
              "77                (:-), nuf)       :-)          nuf           1        626   \n",
              "78            (south, korea)     south        korea           1          2   \n",
              "79               (to, drink)        to        drink           1        996   \n",
              "80                  (in, az)        in           az           1        408   \n",
              "81             (us, realize)        us      realize           1        104   \n",
              "82             (that, orhan)      that        orhan           1        256   \n",
              "83              (garlic, in)    garlic           in           1          1   \n",
              "84              (:), kyunke)        :)       kyunke           1       3245   \n",
              "85         (this, chocolate)      this    chocolate           1        251   \n",
              "86          (cuz, everytime)       cuz    everytime           1          4   \n",
              "87           (expelled, :-))  expelled          :-)           2          2   \n",
              "88              (days, this)      days         this           1         36   \n",
              "89              (it's, like)      it's         like           3        146   \n",
              "90             (tato, bulat)      tato        bulat           1          1   \n",
              "91            (teasers, :-))   teasers          :-)           1          1   \n",
              "92              (should, :))    should           :)           1         27   \n",
              "93            (kik, theoper)       kik      theoper           1         11   \n",
              "94           (problem, i'll)   problem         i'll           1         15   \n",
              "95                  (me, if)        me           if           4        324   \n",
              "96           (feeling, here)   feeling         here           1         15   \n",
              "97           (the, corridor)       the     corridor           1        948   \n",
              "98               (Ô∏è, sorted)         Ô∏è       sorted           1         19   \n",
              "99              (back, soon)      back         soon           1        155   \n",
              "100             (give, dear)      give         dear           1         37   \n",
              "101      (almost, responses)    almost    responses           1         10   \n",
              "102          (i, absolutely)         i   absolutely           1        999   \n",
              "103              (rod, tame)       rod         tame           1          1   \n",
              "104              (board, so)     board           so           1          5   \n",
              "105             (lendal, :))    lendal           :)           1          1   \n",
              "106              (today, a4)     today           a4           1        108   \n",
              "107              (again, to)     again           to           1         59   \n",
              "108  (airforce, battlefield)  airforce  battlefield           1          2   \n",
              "109               (keep, up)      keep           up           5         55   \n",
              "110               (tour, :))      tour           :)           1          8   \n",
              "111     (tearout, selektion)   tearout    selektion           1          1   \n",
              "112                 (or, an)        or           an           5         60   \n",
              "113             (tablet, is)    tablet           is           1          1   \n",
              "114           (agree, sarah)     agree        sarah           1         11   \n",
              "115              (in, today)        in        today           1        408   \n",
              "116               (to, wrap)        to         wrap           1        996   \n",
              "117          (views, thanks)     views       thanks           1          7   \n",
              "118                (egg, on)       egg           on           1          2   \n",
              "119               (that, we)      that           we           5        256   \n",
              "\n",
              "     word2Freq  bigramProb  word1Prob  word2Prob        pmi  \n",
              "70           7    0.000021   0.000214   0.000150   6.504374  \n",
              "71          24    0.000021   0.006929   0.000513   1.794072  \n",
              "72         626    0.000107   0.002566   0.013387   1.135465  \n",
              "73           8    0.000021   0.001283   0.000171   4.579083  \n",
              "74           1    0.000021   0.013237   0.000021   4.324764  \n",
              "75          42    0.000021   0.004491   0.000898   1.668092  \n",
              "76           2    0.000021   0.008105   0.000043   4.122186  \n",
              "77           1    0.000021   0.013387   0.000021   4.313519  \n",
              "78           3    0.000021   0.000043   0.000064   8.961109  \n",
              "79           6    0.000021   0.021299   0.000128   2.057362  \n",
              "80           1    0.000021   0.008725   0.000021   4.741602  \n",
              "81           2    0.000021   0.002224   0.000043   5.415331  \n",
              "82           1    0.000021   0.005474   0.000021   5.207692  \n",
              "83         408    0.000021   0.000021   0.008725   4.741602  \n",
              "84           1    0.000021   0.069392   0.000021   2.667998  \n",
              "85           7    0.000021   0.005367   0.000150   3.281506  \n",
              "86           3    0.000021   0.000086   0.000064   8.267962  \n",
              "87         626    0.000043   0.000043   0.013387   4.313519  \n",
              "88         251    0.000021   0.000770   0.005367   1.643897  \n",
              "89         210    0.000064   0.003122   0.004491   1.520767  \n",
              "90           1    0.000021   0.000021   0.000021  10.752869  \n",
              "91         626    0.000021   0.000021   0.013387   4.313519  \n",
              "92        3245    0.000021   0.000577   0.069392  -0.627839  \n",
              "93           1    0.000021   0.000235   0.000021   8.354974  \n",
              "94          86    0.000021   0.000321   0.001839   3.590471  \n",
              "95         168    0.000086   0.006929   0.003593   1.234456  \n",
              "96          93    0.000021   0.000321   0.001989   3.512219  \n",
              "97           1    0.000021   0.020272   0.000021   3.898514  \n",
              "98           5    0.000021   0.000406   0.000107   6.198992  \n",
              "99          42    0.000021   0.003315   0.000898   1.971774  \n",
              "100         18    0.000021   0.000791   0.000385   4.251579  \n",
              "101          3    0.000021   0.000214   0.000064   7.351672  \n",
              "102          7    0.000021   0.021363   0.000150   1.900204  \n",
              "103          1    0.000021   0.000021   0.000021  10.752869  \n",
              "104        270    0.000021   0.000107   0.005774   3.545009  \n",
              "105       3245    0.000021   0.000021   0.069392   2.667998  \n",
              "106          1    0.000021   0.002310   0.000021   6.070738  \n",
              "107        996    0.000021   0.001262   0.021299  -0.228416  \n",
              "108          1    0.000021   0.000043   0.000021  10.059722  \n",
              "109        137    0.000107   0.001176   0.002930   3.434993  \n",
              "110       3245    0.000021   0.000171   0.069392   0.588557  \n",
              "111          1    0.000021   0.000021   0.000021  10.752869  \n",
              "112        114    0.000107   0.001283   0.002438   3.531764  \n",
              "113        399    0.000021   0.000021   0.008532   4.763908  \n",
              "114          4    0.000021   0.000235   0.000086   6.968679  \n",
              "115        108    0.000021   0.008725   0.002310   0.059471  \n",
              "116          2    0.000021   0.021299   0.000043   3.155975  \n",
              "117        364    0.000021   0.000150   0.007784   2.909805  \n",
              "118        298    0.000021   0.000043   0.006373   4.362628  \n",
              "119        209    0.000107   0.005474   0.004469   1.474795  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1707e3f2-567a-4a50-9438-c939d72a469a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bigram</th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>bigramFreq</th>\n",
              "      <th>word1Freq</th>\n",
              "      <th>word2Freq</th>\n",
              "      <th>bigramProb</th>\n",
              "      <th>word1Prob</th>\n",
              "      <th>word2Prob</th>\n",
              "      <th>pmi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>(almost, views)</td>\n",
              "      <td>almost</td>\n",
              "      <td>views</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>6.504374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>(me, ;))</td>\n",
              "      <td>me</td>\n",
              "      <td>;)</td>\n",
              "      <td>1</td>\n",
              "      <td>324</td>\n",
              "      <td>24</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.006929</td>\n",
              "      <td>0.000513</td>\n",
              "      <td>1.794072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>(now, :-))</td>\n",
              "      <td>now</td>\n",
              "      <td>:-)</td>\n",
              "      <td>5</td>\n",
              "      <td>120</td>\n",
              "      <td>626</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.002566</td>\n",
              "      <td>0.013387</td>\n",
              "      <td>1.135465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>(or, blue)</td>\n",
              "      <td>or</td>\n",
              "      <td>blue</td>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.001283</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>4.579083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>(for, tonights)</td>\n",
              "      <td>for</td>\n",
              "      <td>tonights</td>\n",
              "      <td>1</td>\n",
              "      <td>619</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.013237</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>4.324764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>(good, start)</td>\n",
              "      <td>good</td>\n",
              "      <td>start</td>\n",
              "      <td>1</td>\n",
              "      <td>210</td>\n",
              "      <td>42</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.004491</td>\n",
              "      <td>0.000898</td>\n",
              "      <td>1.668092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>(of, maple)</td>\n",
              "      <td>of</td>\n",
              "      <td>maple</td>\n",
              "      <td>1</td>\n",
              "      <td>379</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.008105</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>4.122186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>(:-), nuf)</td>\n",
              "      <td>:-)</td>\n",
              "      <td>nuf</td>\n",
              "      <td>1</td>\n",
              "      <td>626</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.013387</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>4.313519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>(south, korea)</td>\n",
              "      <td>south</td>\n",
              "      <td>korea</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>8.961109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>(to, drink)</td>\n",
              "      <td>to</td>\n",
              "      <td>drink</td>\n",
              "      <td>1</td>\n",
              "      <td>996</td>\n",
              "      <td>6</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.021299</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>2.057362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>(in, az)</td>\n",
              "      <td>in</td>\n",
              "      <td>az</td>\n",
              "      <td>1</td>\n",
              "      <td>408</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.008725</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>4.741602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>(us, realize)</td>\n",
              "      <td>us</td>\n",
              "      <td>realize</td>\n",
              "      <td>1</td>\n",
              "      <td>104</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>5.415331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>(that, orhan)</td>\n",
              "      <td>that</td>\n",
              "      <td>orhan</td>\n",
              "      <td>1</td>\n",
              "      <td>256</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.005474</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>5.207692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>(garlic, in)</td>\n",
              "      <td>garlic</td>\n",
              "      <td>in</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>408</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.008725</td>\n",
              "      <td>4.741602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>(:), kyunke)</td>\n",
              "      <td>:)</td>\n",
              "      <td>kyunke</td>\n",
              "      <td>1</td>\n",
              "      <td>3245</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.069392</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>2.667998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>(this, chocolate)</td>\n",
              "      <td>this</td>\n",
              "      <td>chocolate</td>\n",
              "      <td>1</td>\n",
              "      <td>251</td>\n",
              "      <td>7</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.005367</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>3.281506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>(cuz, everytime)</td>\n",
              "      <td>cuz</td>\n",
              "      <td>everytime</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>8.267962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>(expelled, :-))</td>\n",
              "      <td>expelled</td>\n",
              "      <td>:-)</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>626</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.013387</td>\n",
              "      <td>4.313519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>(days, this)</td>\n",
              "      <td>days</td>\n",
              "      <td>this</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>251</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000770</td>\n",
              "      <td>0.005367</td>\n",
              "      <td>1.643897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>(it's, like)</td>\n",
              "      <td>it's</td>\n",
              "      <td>like</td>\n",
              "      <td>3</td>\n",
              "      <td>146</td>\n",
              "      <td>210</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.003122</td>\n",
              "      <td>0.004491</td>\n",
              "      <td>1.520767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>(tato, bulat)</td>\n",
              "      <td>tato</td>\n",
              "      <td>bulat</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>10.752869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>(teasers, :-))</td>\n",
              "      <td>teasers</td>\n",
              "      <td>:-)</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>626</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.013387</td>\n",
              "      <td>4.313519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>(should, :))</td>\n",
              "      <td>should</td>\n",
              "      <td>:)</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>3245</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>0.069392</td>\n",
              "      <td>-0.627839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>(kik, theoper)</td>\n",
              "      <td>kik</td>\n",
              "      <td>theoper</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>8.354974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>(problem, i'll)</td>\n",
              "      <td>problem</td>\n",
              "      <td>i'll</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.001839</td>\n",
              "      <td>3.590471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>(me, if)</td>\n",
              "      <td>me</td>\n",
              "      <td>if</td>\n",
              "      <td>4</td>\n",
              "      <td>324</td>\n",
              "      <td>168</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.006929</td>\n",
              "      <td>0.003593</td>\n",
              "      <td>1.234456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>(feeling, here)</td>\n",
              "      <td>feeling</td>\n",
              "      <td>here</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>93</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.001989</td>\n",
              "      <td>3.512219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>(the, corridor)</td>\n",
              "      <td>the</td>\n",
              "      <td>corridor</td>\n",
              "      <td>1</td>\n",
              "      <td>948</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.020272</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>3.898514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>(Ô∏è, sorted)</td>\n",
              "      <td>Ô∏è</td>\n",
              "      <td>sorted</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000406</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>6.198992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>(back, soon)</td>\n",
              "      <td>back</td>\n",
              "      <td>soon</td>\n",
              "      <td>1</td>\n",
              "      <td>155</td>\n",
              "      <td>42</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.003315</td>\n",
              "      <td>0.000898</td>\n",
              "      <td>1.971774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>(give, dear)</td>\n",
              "      <td>give</td>\n",
              "      <td>dear</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>18</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000791</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>4.251579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>(almost, responses)</td>\n",
              "      <td>almost</td>\n",
              "      <td>responses</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>7.351672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>(i, absolutely)</td>\n",
              "      <td>i</td>\n",
              "      <td>absolutely</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>7</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.021363</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>1.900204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>(rod, tame)</td>\n",
              "      <td>rod</td>\n",
              "      <td>tame</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>10.752869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>(board, so)</td>\n",
              "      <td>board</td>\n",
              "      <td>so</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>270</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.005774</td>\n",
              "      <td>3.545009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>(lendal, :))</td>\n",
              "      <td>lendal</td>\n",
              "      <td>:)</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3245</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.069392</td>\n",
              "      <td>2.667998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>(today, a4)</td>\n",
              "      <td>today</td>\n",
              "      <td>a4</td>\n",
              "      <td>1</td>\n",
              "      <td>108</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.002310</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>6.070738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>(again, to)</td>\n",
              "      <td>again</td>\n",
              "      <td>to</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>996</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.001262</td>\n",
              "      <td>0.021299</td>\n",
              "      <td>-0.228416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>(airforce, battlefield)</td>\n",
              "      <td>airforce</td>\n",
              "      <td>battlefield</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>10.059722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>(keep, up)</td>\n",
              "      <td>keep</td>\n",
              "      <td>up</td>\n",
              "      <td>5</td>\n",
              "      <td>55</td>\n",
              "      <td>137</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.001176</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>3.434993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>(tour, :))</td>\n",
              "      <td>tour</td>\n",
              "      <td>:)</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3245</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.069392</td>\n",
              "      <td>0.588557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>(tearout, selektion)</td>\n",
              "      <td>tearout</td>\n",
              "      <td>selektion</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>10.752869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>(or, an)</td>\n",
              "      <td>or</td>\n",
              "      <td>an</td>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>114</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.001283</td>\n",
              "      <td>0.002438</td>\n",
              "      <td>3.531764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>(tablet, is)</td>\n",
              "      <td>tablet</td>\n",
              "      <td>is</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>399</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.008532</td>\n",
              "      <td>4.763908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>(agree, sarah)</td>\n",
              "      <td>agree</td>\n",
              "      <td>sarah</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>6.968679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>(in, today)</td>\n",
              "      <td>in</td>\n",
              "      <td>today</td>\n",
              "      <td>1</td>\n",
              "      <td>408</td>\n",
              "      <td>108</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.008725</td>\n",
              "      <td>0.002310</td>\n",
              "      <td>0.059471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>(to, wrap)</td>\n",
              "      <td>to</td>\n",
              "      <td>wrap</td>\n",
              "      <td>1</td>\n",
              "      <td>996</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.021299</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>3.155975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>(views, thanks)</td>\n",
              "      <td>views</td>\n",
              "      <td>thanks</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>364</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.007784</td>\n",
              "      <td>2.909805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>(egg, on)</td>\n",
              "      <td>egg</td>\n",
              "      <td>on</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>298</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.006373</td>\n",
              "      <td>4.362628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>(that, we)</td>\n",
              "      <td>that</td>\n",
              "      <td>we</td>\n",
              "      <td>5</td>\n",
              "      <td>256</td>\n",
              "      <td>209</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.005474</td>\n",
              "      <td>0.004469</td>\n",
              "      <td>1.474795</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1707e3f2-567a-4a50-9438-c939d72a469a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1707e3f2-567a-4a50-9438-c939d72a469a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1707e3f2-567a-4a50-9438-c939d72a469a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Analize Bigrams from Negative Tweets\n",
        "#@markdown ‚ö° Run this cell to filter the different bigrams, specitying minimum values for PMI and Bigram Frequency.\n",
        "\n",
        "# Interactive Controls\n",
        "minPMI = FloatSlider(min=-2, max=11, step=0.5, value=-2,description='Min PMI',layout=Layout(width='500px'))\n",
        "minBigramFreq = FloatSlider(min=1, max=150, step=1, value=1,description='Min Bigram Freq',layout=Layout(width='500px'))\n",
        "WordsToRemove = \":(,and,at,a,an,follow,please\" #@param {type:\"string\"}\n",
        "\n",
        "@interact\n",
        "def filterPositive(minPMI=minPMI, minBigramFreq=minBigramFreq):\n",
        "  negativeBigrams = getBigramIndicators(negativeTweets)\n",
        "  stopWords = WordsToRemove.replace(' ','').split(',')\n",
        "  df = filterBigrams(negativeBigrams, minPMI, minBigramFreq)\n",
        "  return df[df.apply(lambda data: filterStopWords(data, stopWords), axis=1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615,
          "referenced_widgets": [
            "1645ad0128094958b991cd10a7c6ee21",
            "432dea32fa944d80af4a5dc2fc3d4efc",
            "fe51921624744c7594a296a2bf1b1209",
            "1caddc2b06e34ee292c5516192ee8275",
            "f6b7cb8fd35f4c10a8a1c83b3451f8e2",
            "a3ac1b819ba84d748d2c6fd6262993f0",
            "31057007f8b74accbba94eba74f5e9cc",
            "83388a990b6d45ec8732d249f388a621",
            "ce4cbd7e4b1e4892abe29e100a45e372",
            "75aa5cf6774643a886bb9b620276efad"
          ]
        },
        "cellView": "form",
        "id": "_-ltPwLlprRO",
        "outputId": "51c7e4a2-4e1d-47e8-d328-17c3d1da1e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "interactive(children=(FloatSlider(value=-2.0, description='Min PMI', layout=Layout(width='500px'), max=11.0, m‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1645ad0128094958b991cd10a7c6ee21"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####### Write your code here #######\n",
        "\n",
        "# Use this cell to explore the negativeTweets to validate that the bigrams you chose are really relevant or not\n",
        "\n",
        "data = negativeTweets\n",
        "data[:10]\n",
        "\n",
        "####################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHyHC5euqYwd",
        "outputId": "53ad6847-89b7-4de1-eaf3-764108c78475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hopeless for tmr :(',\n",
              " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
              " '@Hegelbon That heart sliding into the waste basket. :(',\n",
              " '‚Äú@ketchBurning: I hate Japanese call him \"bani\" :( :(‚Äù\\n\\nMe too',\n",
              " 'Dang starting next week I have \"work\" :(',\n",
              " \"oh god, my babies' faces :( https://t.co/9fcwGvaki0\",\n",
              " '@RileyMcDonough make me smile :((',\n",
              " '@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln',\n",
              " 'why?:(\"@tahuodyy: sialan:( https://t.co/Hv1i0xcrL2\"',\n",
              " 'Athabasca glacier was there in #1948 :-( #athabasca #glacier #jasper #jaspernationalpark #alberta #explorealberta #‚Ä¶ http://t.co/dZZdqmf7Cz']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####### Write your code here #######\n",
        "\n",
        "# Use this cell to explore the DataFrame that contains the Bigrams present in the Negative Tweets\n",
        "\n",
        "df = negativeBigrams\n",
        "df[350:380]\n",
        "\n",
        "####################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "dtN8u931rImd",
        "outputId": "e3056625-45e0-44f4-81f2-3596b6f8c6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 bigram     word1      word2  bigramFreq  word1Freq  \\\n",
              "350       (travel, :-()    travel        :-(           1          9   \n",
              "351  (simpson, concert)   simpson    concert           1          2   \n",
              "352     (my, co-worker)        my  co-worker           1        745   \n",
              "353       (agessss, :()   agessss         :(           1          1   \n",
              "354      (photo, taken)     photo      taken           1         12   \n",
              "355          (out, his)       out        his           1        120   \n",
              "356     (the, argument)       the   argument           1        921   \n",
              "357        (hav, phone)       hav      phone           1          2   \n",
              "358         (naomi, :()     naomi         :(           1          1   \n",
              "359        (need, ouat)      need       ouat           1         96   \n",
              "360          (bad, but)       bad        but           1         69   \n",
              "361          (sea, and)       sea        and           1          1   \n",
              "362           (:-(, to)       :-(         to           3        501   \n",
              "363      (amazing, day)   amazing        day           1         16   \n",
              "364         (time, :-()      time        :-(           3        145   \n",
              "365      (rarely, make)    rarely       make           1          1   \n",
              "366            (gyu, y)       gyu          y           1          4   \n",
              "367       (big, estate)       big     estate           1         21   \n",
              "368          (gyu, :-()       gyu        :-(           1          4   \n",
              "369       (longer, ...)    longer        ...           1         12   \n",
              "370    (terrible, time)  terrible       time           1         13   \n",
              "371           (:(, wut)        :(        wut           1       4584   \n",
              "372          (lol, are)       lol        are           1         43   \n",
              "373          (:(, woke)        :(       woke           4       4584   \n",
              "374        (the, books)       the      books           2        921   \n",
              "375           (u, >:-()         u       >:-(           1        196   \n",
              "376        (from, your)      from       your           4         92   \n",
              "377        (:(, unable)        :(     unable           1       4584   \n",
              "378         (mom, what)       mom       what           1         13   \n",
              "379        (things, we)    things         we           2         23   \n",
              "\n",
              "     word2Freq  bigramProb  word1Prob  word2Prob       pmi  \n",
              "350        501    0.000019   0.000167   0.009286  2.482056  \n",
              "351          9    0.000019   0.000037   0.000167  8.005515  \n",
              "352          1    0.000019   0.013808   0.000019  4.282503  \n",
              "353       4584    0.000019   0.000019   0.084963  2.465560  \n",
              "354          9    0.000019   0.000222   0.000167  6.213756  \n",
              "355         39    0.000019   0.002224   0.000723  2.444834  \n",
              "356          1    0.000019   0.017070   0.000019  4.070427  \n",
              "357         35    0.000019   0.000037   0.000649  6.647392  \n",
              "358       4584    0.000019   0.000019   0.084963  2.465560  \n",
              "359          1    0.000019   0.001779   0.000019  6.331539  \n",
              "360        441    0.000019   0.001279   0.008174  0.572736  \n",
              "361        718    0.000019   0.000019   0.013308  4.319418  \n",
              "362       1097    0.000056   0.009286   0.020333 -1.222441  \n",
              "363         97    0.000019   0.000297   0.001798  3.548587  \n",
              "364        501    0.000056   0.002688   0.009286  0.801160  \n",
              "365         56    0.000019   0.000019   0.001038  6.870535  \n",
              "366         12    0.000019   0.000074   0.000222  7.024686  \n",
              "367          1    0.000019   0.000389   0.000019  7.851365  \n",
              "368        501    0.000019   0.000074   0.009286  3.292987  \n",
              "369        331    0.000019   0.000222   0.006135  2.608862  \n",
              "370        145    0.000019   0.000241   0.002688  3.354204  \n",
              "371          1    0.000019   0.084963   0.000019  2.465560  \n",
              "372        171    0.000019   0.000797   0.003169  1.993023  \n",
              "373         14    0.000074   0.084963   0.000259  1.212797  \n",
              "374          5    0.000037   0.017070   0.000093  3.154136  \n",
              "375          4    0.000019   0.003633   0.000074  4.231478  \n",
              "376        137    0.000074   0.001705   0.002539  2.840412  \n",
              "377          3    0.000019   0.084963   0.000056  1.366948  \n",
              "378        128    0.000019   0.000241   0.002372  3.478907  \n",
              "379        163    0.000037   0.000426   0.003021  3.359790  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cabef360-842b-4be6-b56a-0bfe676d098c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bigram</th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>bigramFreq</th>\n",
              "      <th>word1Freq</th>\n",
              "      <th>word2Freq</th>\n",
              "      <th>bigramProb</th>\n",
              "      <th>word1Prob</th>\n",
              "      <th>word2Prob</th>\n",
              "      <th>pmi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>(travel, :-()</td>\n",
              "      <td>travel</td>\n",
              "      <td>:-(</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>501</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.009286</td>\n",
              "      <td>2.482056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351</th>\n",
              "      <td>(simpson, concert)</td>\n",
              "      <td>simpson</td>\n",
              "      <td>concert</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>8.005515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>(my, co-worker)</td>\n",
              "      <td>my</td>\n",
              "      <td>co-worker</td>\n",
              "      <td>1</td>\n",
              "      <td>745</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.013808</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>4.282503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>(agessss, :()</td>\n",
              "      <td>agessss</td>\n",
              "      <td>:(</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4584</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.084963</td>\n",
              "      <td>2.465560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>(photo, taken)</td>\n",
              "      <td>photo</td>\n",
              "      <td>taken</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>6.213756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>(out, his)</td>\n",
              "      <td>out</td>\n",
              "      <td>his</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>39</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>0.000723</td>\n",
              "      <td>2.444834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>(the, argument)</td>\n",
              "      <td>the</td>\n",
              "      <td>argument</td>\n",
              "      <td>1</td>\n",
              "      <td>921</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.017070</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>4.070427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>(hav, phone)</td>\n",
              "      <td>hav</td>\n",
              "      <td>phone</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000649</td>\n",
              "      <td>6.647392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>(naomi, :()</td>\n",
              "      <td>naomi</td>\n",
              "      <td>:(</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4584</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.084963</td>\n",
              "      <td>2.465560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>(need, ouat)</td>\n",
              "      <td>need</td>\n",
              "      <td>ouat</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.001779</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>6.331539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>(bad, but)</td>\n",
              "      <td>bad</td>\n",
              "      <td>but</td>\n",
              "      <td>1</td>\n",
              "      <td>69</td>\n",
              "      <td>441</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.001279</td>\n",
              "      <td>0.008174</td>\n",
              "      <td>0.572736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>(sea, and)</td>\n",
              "      <td>sea</td>\n",
              "      <td>and</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>718</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.013308</td>\n",
              "      <td>4.319418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>(:-(, to)</td>\n",
              "      <td>:-(</td>\n",
              "      <td>to</td>\n",
              "      <td>3</td>\n",
              "      <td>501</td>\n",
              "      <td>1097</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.009286</td>\n",
              "      <td>0.020333</td>\n",
              "      <td>-1.222441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>(amazing, day)</td>\n",
              "      <td>amazing</td>\n",
              "      <td>day</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>97</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000297</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>3.548587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>(time, :-()</td>\n",
              "      <td>time</td>\n",
              "      <td>:-(</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>501</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.002688</td>\n",
              "      <td>0.009286</td>\n",
              "      <td>0.801160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>(rarely, make)</td>\n",
              "      <td>rarely</td>\n",
              "      <td>make</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>56</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.001038</td>\n",
              "      <td>6.870535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>(gyu, y)</td>\n",
              "      <td>gyu</td>\n",
              "      <td>y</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>7.024686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>(big, estate)</td>\n",
              "      <td>big</td>\n",
              "      <td>estate</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>7.851365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>(gyu, :-()</td>\n",
              "      <td>gyu</td>\n",
              "      <td>:-(</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>501</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.009286</td>\n",
              "      <td>3.292987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>(longer, ...)</td>\n",
              "      <td>longer</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>331</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.006135</td>\n",
              "      <td>2.608862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370</th>\n",
              "      <td>(terrible, time)</td>\n",
              "      <td>terrible</td>\n",
              "      <td>time</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>145</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.002688</td>\n",
              "      <td>3.354204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>(:(, wut)</td>\n",
              "      <td>:(</td>\n",
              "      <td>wut</td>\n",
              "      <td>1</td>\n",
              "      <td>4584</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.084963</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>2.465560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>(lol, are)</td>\n",
              "      <td>lol</td>\n",
              "      <td>are</td>\n",
              "      <td>1</td>\n",
              "      <td>43</td>\n",
              "      <td>171</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.003169</td>\n",
              "      <td>1.993023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>(:(, woke)</td>\n",
              "      <td>:(</td>\n",
              "      <td>woke</td>\n",
              "      <td>4</td>\n",
              "      <td>4584</td>\n",
              "      <td>14</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.084963</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>1.212797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>(the, books)</td>\n",
              "      <td>the</td>\n",
              "      <td>books</td>\n",
              "      <td>2</td>\n",
              "      <td>921</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.017070</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>3.154136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>(u, &gt;:-()</td>\n",
              "      <td>u</td>\n",
              "      <td>&gt;:-(</td>\n",
              "      <td>1</td>\n",
              "      <td>196</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.003633</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>4.231478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>(from, your)</td>\n",
              "      <td>from</td>\n",
              "      <td>your</td>\n",
              "      <td>4</td>\n",
              "      <td>92</td>\n",
              "      <td>137</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.001705</td>\n",
              "      <td>0.002539</td>\n",
              "      <td>2.840412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>(:(, unable)</td>\n",
              "      <td>:(</td>\n",
              "      <td>unable</td>\n",
              "      <td>1</td>\n",
              "      <td>4584</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.084963</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>1.366948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>(mom, what)</td>\n",
              "      <td>mom</td>\n",
              "      <td>what</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>128</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.002372</td>\n",
              "      <td>3.478907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>(things, we)</td>\n",
              "      <td>things</td>\n",
              "      <td>we</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>163</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000426</td>\n",
              "      <td>0.003021</td>\n",
              "      <td>3.359790</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cabef360-842b-4be6-b56a-0bfe676d098c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cabef360-842b-4be6-b56a-0bfe676d098c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cabef360-842b-4be6-b56a-0bfe676d098c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ÅâÔ∏è **Question (30 Points)** üßê\n",
        "\n",
        "**Part 1:** Important Bigrams in Positive Tweets ‚ö†Ô∏è  \n",
        "Which are the 10 most important bigrams in positive tweets?\n",
        "```\n",
        "(\"good\", \"luck\")\n",
        "(\"can't\",\"wait\")\n",
        "(\"final\",\"design\")\n",
        "(\"happy\",\"birthday\")\n",
        "(\"display\",\"enabled\")\n",
        "(\"an\",\"intelectual\")\n",
        "(\"south\",\"korea\")\n",
        "(\"light\",\"bulbs\")\n",
        "(\"kind\",\"words\")\n",
        "(\"someone\",\"cares\")\n",
        "```\n",
        "\n",
        "**Part 2:** Obtaining your Positive Bigrams ‚ö†Ô∏è  \n",
        "How did you obtained this list of bigrams? Did you do some extra filtering?\n",
        "Explain the process that you performed to get your results.\n",
        "```\n",
        "I began to obtain my list of bigrams by removing some stop words and emojis\n",
        "from the list. Next, I examined the frequency of the bigrams and their\n",
        "pointwise mutual information (PMI). I selected samples which had high PMI,\n",
        "as it showed they words were mutual. In some cases the Bigram Frequency was\n",
        "high and other times low for my results but the PMI stayed reletively high.\n",
        "An example of this can be (\"an\",\"intelectual\") where the Bigram Frequency was\n",
        "one but the PMI was above eight.\n",
        "```\n",
        "\n",
        "**Part 3:** Important Bigrams in Negative Tweets ‚ö†Ô∏è  \n",
        "Which are the 10 most important bigrams in negative tweets?\n",
        "\n",
        "```\n",
        "(\"come\",\"back\")\n",
        "(\"please\",\"follow\")\n",
        "(\"goodbye\",\"stage\")\n",
        "(\"last\",\"night\")\n",
        "(\"ice\",\"cream\")\n",
        "(\"power\",\"station\")\n",
        "(\"ain't\",\"leaving\")\n",
        "(\"artists\",\"music\")\n",
        "(\"forgetting\",\"you're\")\n",
        "(\"can't\",\"sleep\")\n",
        "```\n",
        "\n",
        "**Part 4:** Obtaining your Negative Bigrams ‚ö†Ô∏è  \n",
        "How did you obtained this list of bigrams? Did you do some extra filtering?\n",
        "Explain the process that you performed to get your results.\n",
        "```\n",
        "Just like with the positive bigrams, I began to obtain my list of bigrams\n",
        "by removing some stop words and emojis from the list. Next, I examined the\n",
        "frequency of the bigrams and their pointwise mutual information (PMI).\n",
        "I selected samples which had high PMI, as it showed they words were mutual.\n",
        "In some cases the Bigram Frequency was high and other times low for my results\n",
        "but the PMI stayed reletively high.\n",
        "\n",
        "The bigram in the negative tweets did not vary from those in the positive.\n",
        "The main differece I came to notice was the emojis and syntaxes and more informal\n",
        "language.\n",
        "```"
      ],
      "metadata": {
        "id": "sHvZPIbIrcv9"
      }
    }
  ]
}